{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import os\nimport time\nimport tensorflow as tf\nimport numpy as np # linear algebra\nimport random\nimport os \nos.environ['PYTHONHASHSEED'] = '11'\nnp.random.seed(22)\nrandom.seed(33)\ntf.set_random_seed(44)\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport gc\n\nfrom keras import initializers, regularizers, constraints\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, Embedding, CuDNNGRU, CuDNNLSTM\nfrom keras.layers import Bidirectional, GlobalMaxPooling1D, Dropout\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.initializers import glorot_uniform\nfrom keras.callbacks import Callback\nfrom keras.models import clone_model\nimport keras.backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = 90000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 50\nt0 = time.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/quora-insincere-questions-classification/train.csv\")\n##test_df = pd.read_csv(\"../input/test.csv\")\nprint(\"Train shape : \",train_df.shape)\n##print(\"Test shape : \",test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extratcing the stopwords in English language"},{"metadata":{"trusted":true},"cell_type":"code","source":"#from nltk import corpus\n#stopwords=corpus.stopwords.words('english')\n\n# fill up the missing values\ntrain_X = train_df[\"question_text\"].fillna(\"_na_\").values\n#test_X = test_df[\"question_text\"].fillna(\"_na_\").values\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Stemming (result includes words not in stopword list)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#from nltk.stem import PorterStemmer\n#from nltk.tokenize import sent_tokenize, word_tokenize\n\ntrain_X = train_df['question_text']\ntrain_X = train_X.tolist()\n\nqid_train = train_df['qid']\nqid_train = qid_train.tolist()\n\n##test_X = test_df['question_text']\n##test_X = test_X.tolist()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Other Preprocesing : converting into lower case, extracting only alphabetical characters"},{"metadata":{},"cell_type":"markdown","source":"Other preprocessing : Tokenization, Padding/Truncating questions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## split to train and val\n# train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)\n\n# Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features,\n                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'’“”')\n\n\ntokenizer.fit_on_texts(train_X)\ntrain_X = tokenizer.texts_to_sequences(train_X)\n\n# Pad the sentences \ntrunc = 'pre'\ntrain_X = pad_sequences(train_X, maxlen=maxlen, truncating=trunc)\n\n##test_X = pad_sequences(test_X, maxlen=maxlen, truncating=trunc)\n\n# Get the target values\ntrain_y = train_df['target'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting data into Train and Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X = train_X[1000000:]\ntrain_X = train_X[:1000000]\ntest_y = train_y[1000000:]\ntrain_y = train_y[:1000000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating Glove Embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_FILE = '../input/quora-insincere-questions-classification/embeddings/glove.840B.300d/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix_1 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix_1[i] = embedding_vector\n\ndel embeddings_index; gc.collect() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating Wiki News Embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_FILE = '../input/quora-insincere-questions-classification/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix_2 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix_2[i] = embedding_vector\n        \ndel embeddings_index; gc.collect() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating Paragram Embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_FILE = '../input/quora-insincere-questions-classification/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix_3 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix_3[i] = embedding_vector\n\ndel embeddings_index; gc.collect() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating Gensim Embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go\nfrom gensim.models import KeyedVectors\n\nEMBEDDING_FILE = '../input/quora-insincere-questions-classification/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\nembeddings_index = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix_4 = (np.random.rand(nb_words, embed_size) - 0.5) / 5.0\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    if word in embeddings_index:\n        embedding_vector = embeddings_index.get_vector(word)\n        embedding_matrix_4[i] = embedding_vector\n        \ndel embeddings_index; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del EMBEDDING_FILE\n#del preprocessed_content1_train\n#del qid_train, stemming_result_train\n#del stopwords\ndel train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculating IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.io import loadmat\nidf = loadmat('../input/idf-mat-file/IDF.mat')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_extra_features=idf['word_extra_features']\nvocab_word_freq=idf['vocab']\nembed_size = 1\nemb_mean, emb_std = word_extra_features.mean(), word_extra_features.std()\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix_5 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    if word in vocab_word_freq:\n        embedding_vector = word_extra_features[list(vocab_word_freq).index(word)]\n        embedding_matrix_5[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Concatinating the embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.concatenate((embedding_matrix_1, embedding_matrix_2, embedding_matrix_3, embedding_matrix_4, embedding_matrix_5), axis=1)\n\ndel idf, embedding_matrix_1, embedding_matrix_2, embedding_matrix_3, embedding_matrix_4, embedding_matrix_5\ngc.collect()\nnp.shape(embedding_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Utilities for Neural Network model"},{"metadata":{},"cell_type":"markdown","source":"Attention Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.engine.topology import Layer\n\nclass Attention(Layer):\n    def __init__(self, step_dim=50,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        \"\"\"\n        Keras Layer that implements an Attention mechanism for temporal data.\n        Supports Masking.\n        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n        # Input shape\n            3D tensor with shape: `(samples, steps, features)`.\n        # Output shape\n            2D tensor with shape: `(samples, features)`.\n        :param kwargs:\n        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n        The dimensions are inferred based on the output shape of the RNN.\n        Example:\n            model.add(LSTM(64, return_sequences=True))\n            model.add(Attention())\n        \"\"\"\n        self.supports_masking = True\n        #self.init = initializations.get('glorot_uniform')\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        # eij = K.dot(x, self.W) TF backend doesn't support it\n\n        # features_dim = self.W.shape[0]\n        # step_dim = x._keras_shape[1]\n\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx())\n\n        # in some cases especially in the early stages of training the sum may be almost zero\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n    #print weigthted_input.shape\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        #return input_shape[0], input_shape[-1]\n        return input_shape[0],  self.features_dim","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Flatten\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import Conv1D, MaxPooling1D\n\ndef create_rnn_model(rnn, lstm, maxlen, embedding, max_features, embed_size,\n                     rnn_dim=64, dense1_dim=100, dense2_dim=50,\n                     embed_trainable=False, seed=123):\n    inp = Input(shape=(maxlen,), dtype='int32')\n    x = Embedding(max_features, embed_size, weights=[embedding],\n                  trainable=embed_trainable)(inp)\n    #x = Conv1D(62,1)(x)\n    #x = MaxPooling1D(2)(x)\n    #x = Bidirectional(rnn(rnn_dim, return_sequences=True,\n                          #kernel_initializer=glorot_uniform(seed=seed)))(x)\n    x = Bidirectional(lstm(rnn_dim, kernel_initializer=glorot_uniform(seed=seed),\n                           return_sequences=True))(x)\n    \n    \n    x = Attention(maxlen)(x)\n    \n    #x = GlobalMaxPooling1D()(x)\n    #x = Flatten()(x)\n    merged = Dense(dense1_dim, activation='relu')(x)\n    merged = BatchNormalization()(merged)\n    preds = Dense(1, activation='sigmoid')(merged)\n    model = Model(inputs=[inp], \\\n        outputs=preds)\n    model.compile(loss='binary_crossentropy',\n        optimizer='adam',\n        metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 4\nnum_val_samples = 200000 \nnum_epochs = 2\nall_scores = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Traaining the model with cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def f1_best(y_val, pred_val):\n    return metrics.f1_score(y_val,pred_val)\n    '''\n    best_f1 = 0\n    best_thresh = 0\n    for thresh in np.linspace(0.2, 0.4, 41):\n        f1 = metrics.f1_score(y_val, (pred_val > thresh).astype(int))\n        if f1 > best_f1:\n            best_f1 = f1\n            best_thresh = thresh\n    return best_f1, best_thresh\n    '''\n\nembed_ids = [list(range(300)), list(range(300, 600)),\n             list(range(600, 900)), list(range(900, 1200))]\nembed_ids_dict = {1: [embed_ids[0], embed_ids[1], embed_ids[2], embed_ids[3]],\n                  2: [embed_ids[0] + embed_ids[1],\n                      embed_ids[0] + embed_ids[2],\n                      embed_ids[0] + embed_ids[3],\n                      embed_ids[1] + embed_ids[2],\n                      embed_ids[1] + embed_ids[3],\n                      embed_ids[2] + embed_ids[3]],\n                  3: [embed_ids[0] + embed_ids[1] + embed_ids[2],\n                      embed_ids[0] + embed_ids[1] + embed_ids[3],\n                      embed_ids[0] + embed_ids[2] + embed_ids[3],\n                      embed_ids[1] + embed_ids[2] + embed_ids[3]],\n                  4: [embed_ids[0] + embed_ids[1] + embed_ids[2] + embed_ids[3]]}\nembed_ids_lst = embed_ids_dict[2]\nembed_size = 1201\n\nrnn = CuDNNGRU\nlstm = CuDNNLSTM\nembed_trainable = False\n\nn_models = 1\n#epochs = 7\nbatch_size = 128\ndense1_dim = rnn_dim = 128\ndense2_dim = 2 * rnn_dim\n\nema_n = int(len(train_y) / batch_size / 10)\ndecay = 0.9\nscores = []\nseed = 101 + 11 * 1\n    \nfor i in range(n_models):\n    t1 = time.time()\n    seed = 101 + 11 * i\n    cols_in_use = embed_ids_lst[i % len(embed_ids_lst)]\n    \n    model = create_rnn_model(rnn, lstm, maxlen, embedding_matrix,\n                             max_features, embed_size,\n                             rnn_dim=rnn_dim,\n                             dense1_dim=dense1_dim,\n                             dense2_dim=dense2_dim,\n                             embed_trainable=embed_trainable,\n                             seed=seed)\n    model.summary()\n\n    \n    #cross_validation \n    for i in range(k):\n        print(f'Processing fold # {i}')\n        val_data = train_X[i * num_val_samples: (i+1) * num_val_samples]\n        val_targets = train_y[i * num_val_samples: (i+1) * num_val_samples]\n    \n        partial_train_data = np.concatenate(\n                                [train_X[:i * num_val_samples],\n                                train_X[(i+1) * num_val_samples:]],\n                                axis=0)\n        partial_train_targets = np.concatenate(\n                                [train_y[:i * num_val_samples],\n                                train_y[(i+1)*num_val_samples:]],\n                                axis=0)\n        res = model.fit(x=partial_train_data,\n                        y=partial_train_targets,\n                        batch_size=batch_size,\n                        epochs = num_epochs,\n                        verbose=1,\n                        validation_data = (val_data,val_targets))\n        print(\"Scores: \", res)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\"../working/model_fscore_nppreprocessing_attention&idf.h5\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculating F1 Score for the test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_avg = f1_best(test_y, np.round(model.predict(test_X)))\nprint(f' F1:{f1_avg:.4f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot the training result"},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\n\ndef plot_history(Exp_history):\n    plt.figure(1)\n    plt.subplot(211)\n    plt.plot(Exp_history.history['acc'])\n    plt.plot(Exp_history.history['val_acc'])\n    #plt.title('accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoches')\n    plt.legend(['train', 'validation'], loc='upper left')\n    plt.subplot(212)\n    plt.plot(Exp_history.history['loss'])\n    plt.plot(Exp_history.history['val_loss'])\n    #plt.title('loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoches')\n    plt.legend(['train', 'validation'], loc='upper left')\n    #plt.savefig(results_path+'/train_history/Exp_train_history_1.png', bbox_inches='tight')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history(res)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}