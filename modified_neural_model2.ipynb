{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":7,"outputs":[{"output_type":"stream","text":"['embeddings', 'train.csv', 'sample_submission.csv', 'test.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import os\nimport time\nimport tensorflow as tf\nimport numpy as np # linear algebra\nimport random\nimport os \nos.environ['PYTHONHASHSEED'] = '11'\nnp.random.seed(22)\nrandom.seed(33)\ntf.set_random_seed(44)\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport gc\n\nfrom keras import initializers, regularizers, constraints\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, Embedding, CuDNNGRU, LSTM, CuDNNLSTM\nfrom keras.layers import Bidirectional, GlobalMaxPooling1D, Dropout\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.initializers import glorot_uniform\nfrom keras.callbacks import Callback\nfrom keras.models import clone_model\nimport keras.backend as K","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\")\n##test_df = pd.read_csv(\"../input/test.csv\")\nprint(\"Train shape : \",train_df.shape)\n##print(\"Test shape : \",test_df.shape)","execution_count":9,"outputs":[{"output_type":"stream","text":"Train shape :  (1306122, 3)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_df","execution_count":22,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extratcing the stopwords in English language"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import corpus\nstopwords=corpus.stopwords.words('english')\n\n# fill up the missing values\ntrain_X = train_df[\"question_text\"].fillna(\"_na_\").values\n#test_X = test_df[\"question_text\"].fillna(\"_na_\").values\n","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del stopwords","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = 90000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 50","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Stemming (result includes words not in stopword list)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\ntrain_X = train_df['question_text']\ntrain_X = train_X.tolist()\n\nqid_train = train_df['qid']\nqid_train = qid_train.tolist()\n\n##test_X = test_df['question_text']\n##test_X = test_X.tolist()\n\n\nstemming_result_train = {}\nps = PorterStemmer()\nfor i in range(0,len(qid_train)):\n    words = word_tokenize(train_X[i])\n    stemming_result_train[qid_train[i]] = []\n    for w in words:\n        if w not in stopwords:\n            stemming_result_train[qid_train[i]].append(ps.stem(w))\n            \n\n##stemming_result_test = []\n##for i in range(len(test_X)):\n    ##for w in test_X[i]:\n        ##stemming_result_test.append(ps.stem(w))\n    \n# stemming_result_test","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del qid_train, stemming_result_train","execution_count":24,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Other Preprocesing : converting into lower case, extracting only alphabetical characters"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nstemmed_content_train = {}\nlowered_content_train = {}\nalphabet_content_train = {}\nfor qid, words in stemming_result_train.items():\n    stemmed_content_train[qid] = \" \".join(str(word) for word in words)\n    lowered_content_train[qid] = stemmed_content_train[qid].lower()\n    alphabet_content_train[qid] = re.sub(r'[^a-zA-Z ]',\"\", lowered_content_train[qid])\n      \n    \n##stemmed_content_test = []\n##lowered_content_test = []\n##alphabet_content_test = []\n##for i in range(len(stemming_result_test)):\n    ##stemmed_content_test.append(\" \".join(str(word) for word in stemming_result_test[i]))\n    ##lowered_content_test.append(stemmed_content_test[i].lower())\n    #alphabet_content[qid] = lowered_content[qid].replace('[^a-z ]','')\n    ##alphabet_content_test.append(re.sub(r'[^a-zA-Z ]',\"\", lowered_content_test[i]))\n    \npreprocessed_content1_train = []\n##preprocessed_content1_test = []\n\nfor qid, text in alphabet_content_train.items():\n    preprocessed_content1_train.append(alphabet_content_train[qid])\n\n##for qid, text in alphabet_content_test.items():\n##   preprocessed_content1_test.append(alphabet_content_test[qid])","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del stemmed_content_train,lowered_content_train,alphabet_content_train, preprocessed_content1_train","execution_count":25,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Other preprocessing : Tokenization, Padding/Truncating questions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## split to train and val\n# train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)\n\n# Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features,\n                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'’“”')\n\n\ntokenizer.fit_on_texts(preprocessed_content1_train)\ntrain_X = tokenizer.texts_to_sequences(preprocessed_content1_train)\n\n# Pad the sentences \ntrunc = 'pre'\ntrain_X = pad_sequences(train_X, maxlen=maxlen, truncating=trunc)\n\n##test_X = pad_sequences(test_X, maxlen=maxlen, truncating=trunc)\n\n# Get the target values\ntrain_y = train_df['target'].values","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del tokenizer","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"7"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Splitting data into Train and Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X = train_X[1000000:]\ntrain_X = train_X[:1000000]\ntest_y = train_y[1000000:]\ntrain_y = train_y[:1000000]","execution_count":15,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating Glove Embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix_1 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix_1[i] = embedding_vector\n\ndel embeddings_index; gc.collect() ","execution_count":16,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n  \"\"\"\n","name":"stderr"},{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Creating Wiki News Embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix_2 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix_2[i] = embedding_vector\n        \ndel embeddings_index; gc.collect() ","execution_count":17,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n  \"\"\"\n","name":"stderr"},{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Creating Paragram Embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix_3 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix_3[i] = embedding_vector\n\ndel embeddings_index; gc.collect() ","execution_count":18,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n  \"\"\"\n","name":"stderr"},{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Creating Gensim Embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go\nfrom gensim.models import KeyedVectors\n\nEMBEDDING_FILE = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\nembeddings_index = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix_4 = (np.random.rand(nb_words, embed_size) - 0.5) / 5.0\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    if word in embeddings_index:\n        embedding_vector = embeddings_index.get_vector(word)\n        embedding_matrix_4[i] = embedding_vector\n        \ndel embeddings_index; gc.collect()","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"del EMBEDDING_FILE","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"69"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Concatinating the embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.concatenate((embedding_matrix_1, embedding_matrix_2, embedding_matrix_3, embedding_matrix_4), axis=1)\n\ndel embedding_matrix_1, embedding_matrix_2, embedding_matrix_3, embedding_matrix_4\ngc.collect()\nnp.shape(embedding_matrix)","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"(90000, 1200)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Utilities for Neural Network model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ExponentialMovingAverage(Callback):\n    def __init__(self, model, decay=0.999, mode='epoch', n=100):\n        \"\"\"\n        mode: 'epoch': Do update_weights every epoch.\n              'batch':                   every n batches.\n        n   :\n        \"\"\"\n        self.decay = decay\n        self.mode = mode\n        self.ema_model = clone_model(model)\n        self.ema_model.set_weights(model.get_weights())\n        self.n = n\n        if self.mode is 'batch':\n            self.cnt = 0\n        self.ema_weights = [K.get_value(w) for w in model.trainable_weights]\n        self.n_weights = len(self.ema_weights)\n        super(ExponentialMovingAverage, self).__init__()\n\n    def on_batch_end(self, batch, logs={}):\n        if self.mode is 'batch':\n            self.cnt += 1\n            if self.cnt % self.n == 0:\n                self.update_weights()\n\n    def on_epoch_end(self, epoch, logs={}):\n        if self.mode is 'epoch':\n            self.update_weights()\n        for var, w in zip(self.ema_model.trainable_weights, self.ema_weights):\n            K.set_value(var, w)\n\n    def update_weights(self):\n        for w_old, var_new in zip(self.ema_weights, self.model.trainable_weights):\n            w_old += (1 - self.decay) * (K.get_value(var_new) - w_old)","execution_count":21,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Attention Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def _time_distributed_dense(x, w, b=None, dropout=None,\n                            input_dim=None, output_dim=None,\n                            timesteps=None, training=None):\n    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n    # Arguments\n        x: input tensor.\n        w: weight matrix.\n        b: optional bias vector.\n        dropout: wether to apply dropout (same dropout mask\n            for every temporal slice of the input).\n        input_dim: integer; optional dimensionality of the input.\n        output_dim: integer; optional dimensionality of the output.\n        timesteps: integer; optional number of timesteps.\n        training: training phase tensor or boolean.\n    # Returns\n        Output tensor.\n    \"\"\"\n    if not input_dim:\n        input_dim = K.shape(x)[2]\n    if not timesteps:\n        timesteps = K.shape(x)[1]\n    if not output_dim:\n        output_dim = K.shape(w)[1]\n\n    if dropout is not None and 0. < dropout < 1.:\n        # apply the same dropout pattern at every timestep\n        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n        dropout_matrix = K.dropout(ones, dropout)\n        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n\n    # collapse time dimension and batch dimension together\n    x = K.reshape(x, (-1, input_dim))\n    x = K.dot(x, w)\n    if b is not None:\n        x = K.bias_add(x, b)\n    # reshape to 3D tensor\n    if K.backend() == 'tensorflow':\n        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n        x.set_shape([None, None, output_dim])\n    else:\n        x = K.reshape(x, (-1, timesteps, output_dim))\n    return x","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers.recurrent import Recurrent\nfrom keras import regularizers, constraints, initializers, activations\nfrom keras.engine import InputSpec \n\nclass AttentionDecoder(Recurrent):\n\n    def __init__(self, units, output_dim,\n                 activation='tanh',\n                 return_probabilities=False,\n                 name='AttentionDecoder',\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        \"\"\"\n        Implements an AttentionDecoder that takes in a sequence encoded by an\n        encoder and outputs the decoded states \n        :param units: dimension of the hidden state and the attention matrices\n        :param output_dim: the number of labels in the output space\n        \"\"\"\n        self.units = units\n        self.output_dim = output_dim\n        self.return_probabilities = return_probabilities\n        self.activation = activations.get(activation)\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(kernel_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        super(AttentionDecoder, self).__init__(**kwargs)\n        self.name = name\n        self.return_sequences = True  # must return sequences\n\n    def build(self, input_shape):\n        \"\"\"\n          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n          for model details that correspond to the matrices here.\n        \"\"\"\n\n        self.batch_size, self.timesteps, self.input_dim = input_shape\n\n        if self.stateful:\n            super(AttentionDecoder, self).reset_states()\n\n        self.states = [None, None]  # y, s\n\n        \"\"\"\n            Matrices for creating the context vector\n        \"\"\"\n\n        self.V_a = self.add_weight(shape=(self.units,),\n                                   name='V_a',\n                                   initializer=self.kernel_initializer,\n                                   regularizer=self.kernel_regularizer,\n                                   constraint=self.kernel_constraint)\n        self.W_a = self.add_weight(shape=(self.units, self.units),\n                                   name='W_a',\n                                   initializer=self.kernel_initializer,\n                                   regularizer=self.kernel_regularizer,\n                                   constraint=self.kernel_constraint)\n        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n                                   name='U_a',\n                                   initializer=self.kernel_initializer,\n                                   regularizer=self.kernel_regularizer,\n                                   constraint=self.kernel_constraint)\n        self.b_a = self.add_weight(shape=(self.units,),\n                                   name='b_a',\n                                   initializer=self.bias_initializer,\n                                   regularizer=self.bias_regularizer,\n                                   constraint=self.bias_constraint)\n        \"\"\"\n            Matrices for the r (reset) gate\n        \"\"\"\n        self.C_r = self.add_weight(shape=(self.input_dim, self.units),\n                                   name='C_r',\n                                   initializer=self.recurrent_initializer,\n                                   regularizer=self.recurrent_regularizer,\n                                   constraint=self.recurrent_constraint)\n        self.U_r = self.add_weight(shape=(self.units, self.units),\n                                   name='U_r',\n                                   initializer=self.recurrent_initializer,\n                                   regularizer=self.recurrent_regularizer,\n                                   constraint=self.recurrent_constraint)\n        self.W_r = self.add_weight(shape=(self.output_dim, self.units),\n                                   name='W_r',\n                                   initializer=self.recurrent_initializer,\n                                   regularizer=self.recurrent_regularizer,\n                                   constraint=self.recurrent_constraint)\n        self.b_r = self.add_weight(shape=(self.units, ),\n                                   name='b_r',\n                                   initializer=self.bias_initializer,\n                                   regularizer=self.bias_regularizer,\n                                   constraint=self.bias_constraint)\n\n        \"\"\"\n            Matrices for the z (update) gate\n        \"\"\"\n        self.C_z = self.add_weight(shape=(self.input_dim, self.units),\n                                   name='C_z',\n                                   initializer=self.recurrent_initializer,\n                                   regularizer=self.recurrent_regularizer,\n                                   constraint=self.recurrent_constraint)\n        self.U_z = self.add_weight(shape=(self.units, self.units),\n                                   name='U_z',\n                                   initializer=self.recurrent_initializer,\n                                   regularizer=self.recurrent_regularizer,\n                                   constraint=self.recurrent_constraint)\n        self.W_z = self.add_weight(shape=(self.output_dim, self.units),\n                                   name='W_z',\n                                   initializer=self.recurrent_initializer,\n                                   regularizer=self.recurrent_regularizer,\n                                   constraint=self.recurrent_constraint)\n        self.b_z = self.add_weight(shape=(self.units, ),\n                                   name='b_z',\n                                   initializer=self.bias_initializer,\n                                   regularizer=self.bias_regularizer,\n                                   constraint=self.bias_constraint)\n        \"\"\"\n            Matrices for the proposal\n        \"\"\"\n        self.C_p = self.add_weight(shape=(self.input_dim, self.units),\n                                   name='C_p',\n                                   initializer=self.recurrent_initializer,\n                                   regularizer=self.recurrent_regularizer,\n                                   constraint=self.recurrent_constraint)\n        self.U_p = self.add_weight(shape=(self.units, self.units),\n                                   name='U_p',\n                                   initializer=self.recurrent_initializer,\n                                   regularizer=self.recurrent_regularizer,\n                                   constraint=self.recurrent_constraint)\n        self.W_p = self.add_weight(shape=(self.output_dim, self.units),\n                                   name='W_p',\n                                   initializer=self.recurrent_initializer,\n                                   regularizer=self.recurrent_regularizer,\n                                   constraint=self.recurrent_constraint)\n        self.b_p = self.add_weight(shape=(self.units, ),\n                                   name='b_p',\n                                   initializer=self.bias_initializer,\n                                   regularizer=self.bias_regularizer,\n                                   constraint=self.bias_constraint)\n        \"\"\"\n            Matrices for making the final prediction vector\n        \"\"\"\n        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n                                   name='C_o',\n                                   initializer=self.recurrent_initializer,\n                                   regularizer=self.recurrent_regularizer,\n                                   constraint=self.recurrent_constraint)\n        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n                                   name='U_o',\n                                   initializer=self.recurrent_initializer,\n                                   regularizer=self.recurrent_regularizer,\n                                   constraint=self.recurrent_constraint)\n        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n                                   name='W_o',\n                                   initializer=self.recurrent_initializer,\n                                   regularizer=self.recurrent_regularizer,\n                                   constraint=self.recurrent_constraint)\n        self.b_o = self.add_weight(shape=(self.output_dim, ),\n                                   name='b_o',\n                                   initializer=self.bias_initializer,\n                                   regularizer=self.bias_regularizer,\n                                   constraint=self.bias_constraint)\n\n        # For creating the initial state:\n        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n                                   name='W_s',\n                                   initializer=self.recurrent_initializer,\n                                   regularizer=self.recurrent_regularizer,\n                                   constraint=self.recurrent_constraint)\n\n        self.input_spec = [\n            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n        self.built = True\n\n    def call(self, x):\n        # store the whole sequence so we can \"attend\" to it at each timestep\n        self.x_seq = x\n\n        # apply the a dense layer over the time dimension of the sequence\n        # do it here because it doesn't depend on any previous steps\n        # thefore we can save computation time:\n        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n                                             input_dim=self.input_dim,\n                                             timesteps=self.timesteps,\n                                             output_dim=self.units)\n\n        return super(AttentionDecoder, self).call(x)\n\n    def get_initial_state(self, inputs):\n        print('inputs shape:', inputs.get_shape())\n\n        # apply the matrix on the first time step to get the initial s0.\n        s0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n\n        # from keras.layers.recurrent to initialize a vector of (batchsize,\n        # output_dim)\n        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n        y0 = K.expand_dims(y0)  # (samples, 1)\n        y0 = K.tile(y0, [1, self.output_dim])\n\n        return [y0, s0]\n\n    def step(self, x, states):\n\n        ytm, stm = states\n\n        # repeat the hidden state to the length of the sequence\n        _stm = K.repeat(stm, self.timesteps)\n\n        # now multiplty the weight matrix with the repeated hidden state\n        _Wxstm = K.dot(_stm, self.W_a)\n\n        # calculate the attention probabilities\n        # this relates how much other timesteps contributed to this one.\n        et = K.dot(activations.tanh(_Wxstm + self._uxpb),\n                   K.expand_dims(self.V_a))\n        at = K.exp(et)\n        at_sum = K.sum(at, axis=1)\n        at_sum_repeated = K.repeat(at_sum, self.timesteps)\n        at /= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n\n        # calculate the context vector\n        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n        # ~~~> calculate new hidden state\n        # first calculate the \"r\" gate:\n\n        rt = activations.sigmoid(\n            K.dot(ytm, self.W_r)\n            + K.dot(stm, self.U_r)\n            + K.dot(context, self.C_r)\n            + self.b_r)\n\n        # now calculate the \"z\" gate\n        zt = activations.sigmoid(\n            K.dot(ytm, self.W_z)\n            + K.dot(stm, self.U_z)\n            + K.dot(context, self.C_z)\n            + self.b_z)\n\n        # calculate the proposal hidden state:\n        s_tp = activations.tanh(\n            K.dot(ytm, self.W_p)\n            + K.dot((rt * stm), self.U_p)\n            + K.dot(context, self.C_p)\n            + self.b_p)\n\n        # new hidden state:\n        st = (1-zt)*stm + zt * s_tp\n\n        yt = activations.softmax(\n            K.dot(ytm, self.W_o)\n            + K.dot(stm, self.U_o)\n            + K.dot(context, self.C_o)\n            + self.b_o)\n\n        if self.return_probabilities:\n            return at, [yt, st]\n        else:\n            return yt, [yt, st]\n\n    def compute_output_shape(self, input_shape):\n        \"\"\"\n            For Keras internal compatability checking\n        \"\"\"\n        if self.return_probabilities:\n            return (None, self.timesteps, self.timesteps)\n        else:\n            return (None, self.timesteps, self.output_dim)\n\n    def get_config(self):\n        \"\"\"\n            For rebuilding models on load time.\n        \"\"\"\n        config = {\n            'output_dim': self.output_dim,\n            'units': self.units,\n            'return_probabilities': self.return_probabilities\n        }\n        base_config = super(AttentionDecoder, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Flatten\n\ndef create_rnn_model(rnn, lstm, maxlen, embedding, max_features, embed_size,\n                     rnn_dim=64, dense1_dim=100, dense2_dim=50,\n                     embed_trainable=False, seed=123):\n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, weights=[embedding],\n                  trainable=embed_trainable)(inp)\n    x = Dense(dense1_dim, activation='relu',\n              kernel_initializer=glorot_uniform(seed=seed))(x)\n    x = Bidirectional(rnn(rnn_dim, return_sequences=True,\n                          kernel_initializer=glorot_uniform(seed=seed)))(x)\n    #x = Attention(maxlen)(x)\n    x = Bidirectional(lstm(rnn_dim, kernel_initializer=glorot_uniform(seed=seed),\n                           return_sequences=True))(x)\n    x = AttentionDecoder(rnn_dim,\n                             name='attention_decoder_1',\n                             output_dim=1,\n                             return_probabilities=False,\n                             trainable=True)(x) \n    ##x =  Flatten()\n    ##x = Dense(1, activation='sigmoid')\n    #x = GlobalMaxPooling1D()(x)\n    #x = Dense(dense2_dim, activation='relu',\n              #kernel_initializer=glorot_uniform(seed=seed))(x)\n    #x = Dense(1, activation='sigmoid',\n              #kernel_initializer=glorot_uniform(seed=seed))(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['acc'])\n    return model","execution_count":71,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 8\nnum_val_samples = 200000 \nnum_epochs = 10\nall_scores = []","execution_count":72,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = np.concatenate([train_y[:i * num_val_samples],\n                train_y[(i+1) * num_val_samples:]],\n                axis=0)","execution_count":69,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp.shape","execution_count":70,"outputs":[{"output_type":"execute_result","execution_count":70,"data":{"text/plain":"(800000,)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def f1_best(y_val, pred_val):\n    best_f1 = 0\n    best_thresh = 0\n    for thresh in np.linspace(0.2, 0.4, 41):\n        f1 = metrics.f1_score(y_val, (pred_val > thresh).astype(int))\n        if f1 > best_f1:\n            best_f1 = f1\n            best_thresh = thresh\n    return best_f1, best_thresh\n\nembed_ids = [list(range(300)), list(range(300, 600)),\n             list(range(600, 900)), list(range(900, 1200))]\nembed_ids_dict = {1: [embed_ids[0], embed_ids[1], embed_ids[2], embed_ids[3]],\n                  2: [embed_ids[0] + embed_ids[1],\n                      embed_ids[0] + embed_ids[2],\n                      embed_ids[0] + embed_ids[3],\n                      embed_ids[1] + embed_ids[2],\n                      embed_ids[1] + embed_ids[3],\n                      embed_ids[2] + embed_ids[3]],\n                  3: [embed_ids[0] + embed_ids[1] + embed_ids[2],\n                      embed_ids[0] + embed_ids[1] + embed_ids[3],\n                      embed_ids[0] + embed_ids[2] + embed_ids[3],\n                      embed_ids[1] + embed_ids[2] + embed_ids[3]],\n                  4: [embed_ids[0] + embed_ids[1] + embed_ids[2] + embed_ids[3]]}\nembed_ids_lst = embed_ids_dict[2]\nembed_size = 600\n\nrnn = CuDNNGRU\nlstm = CuDNNLSTM\nembed_trainable = False\n\nn_models = 6\n#epochs = 7\nbatch_size = 512\ndense1_dim = rnn_dim = 128\ndense2_dim = 2 * rnn_dim\n\nema_n = int(len(train_y) / batch_size / 10)\ndecay = 0.9\nscores = []\n\n#pred_avg = np.zeros((len(val_y), 1))\n#pred_test_avg = np.zeros((test_df.shape[0], 1))\nfor i in range(n_models):\n    t1 = time.time()\n    seed = 101 + 11 * i\n    cols_in_use = embed_ids_lst[i % len(embed_ids_lst)]\n    model = create_rnn_model(rnn, lstm, maxlen, embedding_matrix[:, cols_in_use],\n                             max_features, embed_size,\n                             rnn_dim=rnn_dim,\n                             dense1_dim=dense1_dim,\n                             dense2_dim=dense2_dim,\n                             embed_trainable=embed_trainable,\n                             seed=seed)\n    model.summary()\n    ema = ExponentialMovingAverage(model, decay=decay, mode='batch', n=ema_n)\n    \n    #cross_validation \n    for i in range(k):\n        print(f'Processing fold # {i}')\n        val_data = train_X[i * num_val_samples: (i+1) * num_val_samples]\n        val_targets = train_y[i * num_val_samples: (i+1) * num_val_samples]\n    \n        partial_train_data = np.concatenate(\n                                [train_X[:i * num_val_samples],\n                                train_X[(i+1) * num_val_samples:]],\n                                axis=0)\n        partial_train_targets = np.concatenate(\n                                [train_y[:i * num_val_samples],\n                                train_y[(i+1)*num_val_samples:]],\n                                axis=0)\n        res = model.fit(x=partial_train_data,\n                        y=partial_train_targets,\n                        batch_size=batch_size,\n                        epochs = num_epochs,\n                        verbose=1,\n                        validation_data = (val_data,val_targets))\n        print(\"Scores: \", res)\n    \n    #model.fit(train_X, train_y, batch_size=batch_size, epochs=epochs,\n              #callbacks=[ema], verbose=0)\n    m = ema.ema_model\n    t_per_epoch = (time.time() - t1) / epochs\n    pred = m.predict([val_X])\n    pred_avg += pred\n    pred_test = m.predict([test_X])\n    pred_test_avg += pred_test\n    f1_one, thresh_one = f1_best(val_y, pred)\n    f1_avg, thresh_avg = f1_best(val_y, pred_avg / (i + 1))\n    nll_one = metrics.log_loss(val_y, pred)\n    nll_avg = metrics.log_loss(val_y, pred_avg / (i + 1))\n    auc_one = metrics.roc_auc_score(val_y, pred)\n    auc_avg = metrics.roc_auc_score(val_y, pred_avg)\n    print(f'  n_model:{i + 1} epoch:{epochs} F1:{f1_avg:.4f} th:{thresh_avg:.3f} ' +\n          f'AUC:{auc_avg:.4f} NLL:{nll_avg:.4f} One:{f1_one:.4f} {auc_one:.4f} {nll_one:.4f} ' +\n          f'Time:{time.time() - t1:.1f}s  {t_per_epoch:.1f}s/epoch')","execution_count":76,"outputs":[{"output_type":"stream","text":"inputs shape: (?, 50, 256)\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_12 (InputLayer)        (None, 50)                0         \n_________________________________________________________________\nembedding_12 (Embedding)     (None, 50, 600)           54000000  \n_________________________________________________________________\ndense_15 (Dense)             (None, 50, 128)           76928     \n_________________________________________________________________\nbidirectional_23 (Bidirectio (None, 50, 256)           198144    \n_________________________________________________________________\nbidirectional_24 (Bidirectio (None, 50, 256)           395264    \n_________________________________________________________________\nattention_decoder_1 (Attenti (None, 50, 1)             230786    \n=================================================================\nTotal params: 54,901,122\nTrainable params: 901,122\nNon-trainable params: 54,000,000\n_________________________________________________________________\ninputs shape: (?, 50, 256)\nProcessing fold # 0\n","name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"Error when checking target: expected attention_decoder_1 to have 3 dimensions, but got array with shape (800000, 1)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-76-14b5bf57f237>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m                         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                         validation_data = (val_data,val_targets))\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Scores: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Error when checking target: expected attention_decoder_1 to have 3 dimensions, but got array with shape (800000, 1)"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test_avg /= n_models\npred_test_avg = (pred_test_avg>thresh_avg).astype(int)\nout_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\nout_df['prediction'] = pred_test_avg\nout_df.to_csv(\"submission.csv\", index=False)\n\nprint(f'Done:{time.time() - t0:.1f}s')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(out_df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}