{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import argparse\nimport json\nimport math\nimport os\nimport sys\nimport unidecode\nimport random\nimport re\nimport time\nimport yaml\nfrom abc import ABCMeta, abstractmethod\nfrom collections import defaultdict, Counter\nfrom copy import deepcopy\nfrom functools import partial\nfrom multiprocessing import Pool\nfrom pathlib import Path\n\nimport nltk\nimport gensim\nimport sklearn\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nfrom gensim.corpora import Dictionary\nfrom gensim.models import KeyedVectors\nfrom gensim.models import Word2Vec, Doc2Vec, FastText\nfrom sklearn import metrics\nfrom torch import nn\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\nfrom tqdm import tqdm\n%load_ext Cython","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ExperimentConfigBuilderBase(metaclass=ABCMeta):\n\n    default_config = None\n\n    def add_args(self, parser):\n        parser.add_argument('--modelfile', '-m', type=Path)\n        parser.add_argument('--outdir-top', type=Path, default=Path('results'))\n        parser.add_argument('--outdir-bottom', type=str, default='default')\n        parser.add_argument('--device', '-g', type=int)\n        parser.add_argument('--test', action='store_true')\n        parser.add_argument('--logging', action='store_true')\n        parser.add_argument('--n-rows', type=int)\n\n        parser.add_argument('--seed', type=int, default=1029)\n        parser.add_argument('--optuna-trials', type=int)\n        parser.add_argument('--gridsearch', action='store_true')\n        parser.add_argument('--holdout', action='store_true')\n        parser.add_argument('--cv', type=int, default=5)\n        parser.add_argument('--cv-part', type=int)\n        parser.add_argument('--processes', type=int, default=2)\n\n        parser.add_argument('--lr', type=float, default=1e-3)\n        parser.add_argument('--batchsize', type=int, default=512)\n        parser.add_argument('--batchsize-valid', type=int, default=1024)\n        parser.add_argument('--scale-batchsize', type=int, nargs='+',\n                            default=[])\n        parser.add_argument('--epochs', type=int, default=5)\n        parser.add_argument('--validate-from', type=int)\n        parser.add_argument('--pos-weight', type=float, default=1.)\n        parser.add_argument('--maxlen', type=float, default=72)\n        parser.add_argument('--vocab-mincount', type=float, default=5)\n        parser.add_argument('--ensembler-n-snapshots', type=int, default=1)\n\n    @abstractmethod\n    def modules(self):\n        raise NotImplementedError()\n\n    def build(self, args=None):\n        assert self.default_config is not None\n        parser = argparse.ArgumentParser()\n        self.add_args(parser)\n        parser.set_defaults(**self.default_config)\n\n        for module in self.modules:\n            module.add_args(parser)\n        config, extra_config = parser.parse_known_args(args)\n\n        for module in self.modules:\n            if hasattr(module, 'add_extra_args'):\n                module.add_extra_args(parser, config)\n\n        if config.test:\n            parser.set_defaults(**dict(\n                n_rows=500,\n                batchsize=64,\n                validate_from=0,\n                epochs=3,\n                cv_part=2,\n                ensembler_test_size=1.,\n            ))\n\n        config = parser.parse_args(args)\n        if config.modelfile is not None:\n            config.outdir = config.outdir_top / config.modelfile.stem \\\n                / config.outdir_bottom\n        else:\n            config.outdir = Path('.')\n\n        return config","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#modules = \"\"\"\n\n\n#\"\"\"\n#os.environ['DATADIR'] = '/kaggle/input'\ndef build_model(config, embedding_matrix, n_sentence_extra_features):\n    embedding = Embedding(config, embedding_matrix)\n    encoder = Encoder(config, embedding.out_size)\n    aggregator = Aggregator(config)\n    mlp = MLP(config, encoder.out_size + n_sentence_extra_features)\n    out = nn.Linear(config.mlp_n_hiddens[-1], 1)\n    lossfunc = nn.BCEWithLogitsLoss()\n\n    return BinaryClassifier(\n        embedding=embedding,\n        encoder=encoder,\n        aggregator=aggregator,\n        mlp=mlp,\n        out=out,\n        lossfunc=lossfunc,\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_qiqc(n_rows=None):\n    #train_df = pd.read_csv(f'{os.environ[\"DATADIR\"]}/train.csv', nrows=n_rows)\n    #submit_df = pd.read_csv(f'{os.environ[\"DATADIR\"]}/test.csv', nrows=n_rows)\n    train_df=pd.read_csv(\"../input/train.csv\", nrows=n_rows)\n    submit_df = pd.read_csv(\"../input/test.csv\",nrows=n_rows)\n    n_labels = {\n        0: (train_df.target == 0).sum(),\n        1: (train_df.target == 1).sum(),\n    }\n    train_df['target'] = train_df.target.astype('f')\n    train_df['weights'] = train_df.target.apply(lambda t: 1 / n_labels[t])\n\n    return train_df, submit_df\n\n\ndef build_datasets(train_df, submit_df, holdout=False, seed=0):\n    submit_dataset = QIQCDataset(submit_df)\n    if holdout:\n        # Train : Test split for holdout training\n        splitter = sklearn.model_selection.StratifiedShuffleSplit(\n            n_splits=1, test_size=0.1, random_state=seed)\n        train_indices, test_indices = list(splitter.split(\n            train_df, train_df.target))[0]\n        train_indices.sort(), test_indices.sort()\n        train_dataset = QIQCDataset(\n            train_df.iloc[train_indices].reset_index(drop=True))\n        test_dataset = QIQCDataset(\n            train_df.iloc[test_indices].reset_index(drop=True))\n    else:\n        train_dataset = QIQCDataset(train_df)\n        test_dataset = QIQCDataset(train_df.head(0))\n\n    return train_dataset, test_dataset, submit_dataset\n\n\nclass QIQCDataset(object):\n\n    def __init__(self, df):\n        self.df = df\n\n    @property\n    def tokens(self):\n        return self.df.tokens.values\n\n    @tokens.setter\n    def tokens(self, tokens):\n        self.df['tokens'] = tokens\n\n    @property\n    def positives(self):\n        return self.df[self.df.target == 1]\n\n    @property\n    def negatives(self):\n        return self.df[self.df.target == 0]\n\n    def build(self, device):\n        self._X = self.tids\n        self.X = torch.Tensor(self._X).type(torch.long).to(device)\n        if 'target' in self.df:\n            self._t = self.df.target[:, None]\n            self._W = self.df.weights\n            self.t = torch.Tensor(self._t).type(torch.float).to(device)\n            self.W = torch.Tensor(self._W).type(torch.float).to(device)\n        if hasattr(self, '_X2'):\n            self.X2 = torch.Tensor(self._X2).type(torch.float).to(device)\n        else:\n            self._X2 = np.zeros((self._X.shape[0], 1), 'f')\n            self.X2 = torch.Tensor(self._X2).type(torch.float).to(device)\n\n    def build_labeled_dataset(self, indices):\n        return torch.utils.data.TensorDataset(\n            self.X[indices], self.X2[indices],\n            self.t[indices], self.W[indices])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Registries for preprocessing\nNORMALIZER_REGISTRY = {}\nTOKENIZER_REGISTRY = {}\nWORD_EMBEDDING_FEATURIZER_REGISTRY = {}\nWORD_EXTRA_FEATURIZER_REGISTRY = {}\nSENTENCE_EXTRA_FEATURIZER_REGISTRY = {}\n\n# Registries for training\nENCODER_REGISTRY = {}\nAGGREGATOR_REGISTRY = {}\nATTENTION_REGISTRY = {}\n\n\ndef register_preprocessor(name):\n    def register_cls(cls):\n        NORMALIZER_REGISTRY[name] = cls\n        return cls\n    return register_cls\n\n\ndef register_tokenizer(name):\n    def register_cls(cls):\n        TOKENIZER_REGISTRY[name] = cls\n        return cls\n    return register_cls\n\n\ndef register_word_embedding_features(name):\n    def register_cls(cls):\n        WORD_EMBEDDING_FEATURIZER_REGISTRY[name] = cls\n        return cls\n    return register_cls\n\n\ndef register_word_extra_features(name):\n    def register_cls(cls):\n        WORD_EXTRA_FEATURIZER_REGISTRY[name] = cls\n        return cls\n    return register_cls\n\n\ndef register_sentence_extra_features(name):\n    def register_cls(cls):\n        SENTENCE_EXTRA_FEATURIZER_REGISTRY[name] = cls\n        return cls\n    return register_cls\n\n\ndef register_encoder(name):\n    def register_cls(cls):\n        ENCODER_REGISTRY[name] = cls\n        return cls\n    return register_cls\n\n\ndef register_aggregator(name):\n    def register_cls(cls):\n        AGGREGATOR_REGISTRY[name] = cls\n        return cls\n    return register_cls\n\n\ndef register_attention(name):\n    def register_cls(cls):\n        ATTENTION_REGISTRY[name] = cls\n        return cls\n    return register_cls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # class UnkWordFeaturizer(object):\n\n# #     def __call__(self, vocab):\n# #         features = vocab.unk.astype('f')\n\n# @register_word_extra_features('unk')\nclass WordVocab(object):\n\n    def __init__(self, mincount=1):\n        self.counter = Counter()\n        self.n_documents = 0\n        self._counters = {}\n        self._n_documents = defaultdict(int)\n        self.mincount = mincount\n#         self.unk =  vocab.unk.astype('f')\n\n    def __len__(self):\n        return len(self.token2id)\n\n    def add_documents(self, documents, name):\n        self._counters[name] = Counter()\n        for document in documents:\n            bow = dict.fromkeys(document, 1)\n            self._counters[name].update(bow)\n            self.counter.update(bow)\n            self.n_documents += 1\n            self._n_documents[name] += 1\n\n    def build(self):\n        counter = dict(self.counter.most_common())\n        self.word_freq = {\n            **{'<PAD>': 0},\n            **counter,\n        }\n        self.token2id = {\n            **{'<PAD>': 0},\n            **{word: i + 1 for i, word in enumerate(counter)}\n        }\n        self.lfq = np.array(list(self.word_freq.values())) < self.mincount\n        self.hfq = ~self.lfq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%cython\nimport re\n\nimport numpy as np\ncimport numpy as np\n\n\ncdef class StringReplacer:\n    cpdef public dict rule\n    cpdef list keys\n    cpdef list values\n    cpdef int n_rules\n\n    def __init__(self, dict rule):\n        self.rule = rule\n        self.keys = list(rule.keys())\n        self.values = list(rule.values())\n        self.n_rules = len(rule)\n\n    def __call__(self, str x):\n        cdef int i\n        for i in range(self.n_rules):\n            if self.keys[i] in x:\n                x = x.replace(self.keys[i], self.values[i])\n        return x\n\n    def __getstate__(self):\n        return (self.rule, self.keys, self.values, self.n_rules)\n\n    def __setstate__(self, state):\n        self.rule, self.keys, self.values, self.n_rules = state\n\n\ncdef class RegExpReplacer:\n    cdef dict rule\n    cdef list keys\n    cdef list values\n    cdef regexp\n    cdef int n_rules\n\n    def __init__(self, dict rule):\n        self.rule = rule\n        self.keys = list(rule.keys())\n        self.values = list(rule.values())\n        self.regexp = re.compile('(%s)' % '|'.join(self.keys))\n        self.n_rules = len(rule)\n\n    @property\n    def rule(self):\n        return self.rule\n\n    def __call__(self, str x):\n        def replace(match):\n            x = match.group(0)\n            if x in self.rule:\n                return self.rule[x]\n            else:\n                for i in range(self.n_rules):\n                    x = re.sub(self.keys[i], self.values[i], x)\n                return x\n        return self.regexp.sub(replace, x)\n\n\ncpdef str cylower(str x):\n    return x.lower()\n\n\nCache = {}\nis_alphabet = re.compile(r'[a-zA-Z]')\n\n\ncpdef str unidecode_weak(str string):\n    \"\"\"Transliterate an Unicode object into an ASCII string\n    >>> unidecode(u\"\\u5317\\u4EB0\")\n    \"Bei Jing \"\n    \"\"\"\n\n    cdef list retval = []\n    cdef int i = 0\n    cdef int n = len(string)\n    cdef str char\n\n    for i in range(n):\n        char = string[i]\n        codepoint = ord(char)\n\n        if codepoint < 0x80: # Basic ASCII\n            retval.append(char)\n            continue\n\n        if codepoint > 0xeffff:\n            continue  # Characters in Private Use Area and above are ignored\n\n        section = codepoint >> 8   # Chop off the last two hex digits\n        position = codepoint % 256 # Last two hex digits\n\n        try:\n            table = Cache[section]\n        except KeyError:\n            try:\n                mod = __import__('unidecode.x%03x'%(section), [], [], ['data'])\n            except ImportError:\n                Cache[section] = None\n                continue   # No match: ignore this character and carry on.\n\n            Cache[section] = table = mod.data\n\n        if table and len(table) > position:\n            if table[position] == '[?]' or is_alphabet.match(table[position]):\n                retval.append(' ' + char + ' ')\n            else:\n                retval.append(table[position])\n\n    return ''.join(retval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PunctSpacer(StringReplacer):\n\n    def __init__(self, edge_only=False):\n        puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', '█', '½', '…', '“', '★', '”', '–', '●', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', '¯', '♦', '¤', '▲', '¸', '¾', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]  # NOQA\n        if edge_only:\n            rule = {\n                **dict([(f' {p}', f' {p} ') for p in puncts]),\n                **dict([(f'{p} ', f' {p} ') for p in puncts]),\n            }\n        else:\n            rule = dict([(p, f' {p} ') for p in puncts])\n        super().__init__(rule)\n\n\nclass NumberReplacer(RegExpReplacer):\n\n    def __init__(self, with_underscore=False):\n        prefix, suffix = '', ''\n        if with_underscore:\n            prefix += ' __'\n            suffix = '__ '\n        rule = {\n            '[0-9]{5,}': f'{prefix}#####{suffix}',\n            '[0-9]{4}': f'{prefix}####{suffix}',\n            '[0-9]{3}': f'{prefix}###{suffix}',\n            '[0-9]{2}': f'{prefix}##{suffix}',\n        }\n        super().__init__(rule)\n\n\nclass KerasFilterReplacer(StringReplacer):\n\n    def __init__(self):\n        filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n        rule = dict([(f, ' ') for f in filters])\n        super().__init__(rule)\n\n\nclass MisspellReplacer(StringReplacer):\n\n    def __init__(self):\n        rule = {\n            \"ain't\": \"is not\",\n            \"aren't\": \"are not\",\n            \"can't\": \"cannot\",\n            \"'cause\": \"because\",\n            \"could've\": \"could have\",\n            \"couldn't\": \"could not\",\n            \"didn't\": \"did not\",\n            \"doesn't\": \"does not\",\n            \"don't\": \"do not\",\n            \"hadn't\": \"had not\",\n            \"hasn't\": \"has not\",\n            \"haven't\": \"have not\",\n            \"he'd\": \"he would\",\n            \"he'll\": \"he will\",\n            \"he's\": \"he is\",\n            \"how'd'y\": \"how do you\",\n            \"how'd\": \"how did\",\n            \"how'll\": \"how will\",\n            \"how's\": \"how is\",\n            \"i'd've\": \"i would have\",\n            \"i'd\": \"i would\",\n            \"i'll've\": \"i will have\",\n            \"i'll\": \"i will\",\n            \"i'm\": \"i am\",\n            \"i've\": \"i have\",\n            \"isn't\": \"is not\",\n            \"it'd've\": \"it would have\",\n            \"it'd\": \"it would\",\n            \"it'll've\": \"it will have\",\n            \"it'll\": \"it will\",\n            \"it's\": \"it is\",\n            \"let's\": \"let us\",\n            \"ma'am\": \"madam\",\n            \"mayn't\": \"may not\",\n            \"might've\": \"might have\",\n            \"mightn't've\": \"might not have\",\n            \"mightn't\": \"might not\",\n            \"must've\": \"must have\",\n            \"mustn't've\": \"must not have\",\n            \"mustn't\": \"must not\",\n            \"needn't've\": \"need not have\",\n            \"needn't\": \"need not\",\n            \"o'clock\": \"of the clock\",\n            \"oughtn't've\": \"ought not have\",\n            \"oughtn't\": \"ought not\",\n            \"shan't've\": \"shall not have\",\n            \"shan't\": \"shall not\",\n            \"sha'n't\": \"shall not\",\n            \"she'd've\": \"she would have\",\n            \"she'd\": \"she would\",\n            \"she'll've\": \"she will have\",\n            \"she'll\": \"she will\",\n            \"she's\": \"she is\",\n            \"should've\": \"should have\",\n            \"shouldn't've\": \"should not have\",\n            \"shouldn't\": \"should not\",\n            \"so've\": \"so have\",\n            \"so's\": \"so as\",\n            \"this's\": \"this is\",\n            \"that'd've\": \"that would have\",\n            \"that'd\": \"that would\",\n            \"that's\": \"that is\",\n            \"there'd've\": \"there would have\",\n            \"there'd\": \"there would\",\n            \"there's\": \"there is\",\n            \"here's\": \"here is\",\n            \"they'd've\": \"they would have\",\n            \"they'd\": \"they would\",\n            \"they'll've\": \"they will have\",\n            \"they'll\": \"they will\",\n            \"they're\": \"they are\",\n            \"they've\": \"they have\",\n            \"to've\": \"to have\",\n            \"wasn't\": \"was not\",\n            \"we'd've\": \"we would have\",\n            \"we'd\": \"we would\",\n            \"we'll've\": \"we will have\",\n            \"we'll\": \"we will\",\n            \"we're\": \"we are\",\n            \"we've\": \"we have\",\n            \"weren't\": \"were not\",\n            \"what'll've\": \"what will have\",\n            \"what'll\": \"what will\",\n            \"what're\": \"what are\",\n            \"what's\": \"what is\",\n            \"what've\": \"what have\",\n            \"when's\": \"when is\",\n            \"when've\": \"when have\",\n            \"where'd\": \"where did\",\n            \"where's\": \"where is\",\n            \"where've\": \"where have\",\n            \"who'll've\": \"who will have\",\n            \"who'll\": \"who will\",\n            \"who's\": \"who is\",\n            \"who've\": \"who have\",\n            \"why's\": \"why is\",\n            \"why've\": \"why have\",\n            \"will've\": \"will have\",\n            \"won't've\": \"will not have\",\n            \"won't\": \"will not\",\n            \"would've\": \"would have\",\n            \"wouldn't've\": \"would not have\",\n            \"wouldn't\": \"would not\",\n            \"y'all'd've\": \"you all would have\",\n            \"y'all'd\": \"you all would\",\n            \"y'all're\": \"you all are\",\n            \"y'all've\": \"you all have\",\n            \"y'all\": \"you all\",\n            \"you'd've\": \"you would have\",\n            \"you'd\": \"you would\",\n            \"you'll've\": \"you will have\",\n            \"you'll\": \"you will\",\n            \"you're\": \"you are\",\n            \"you've\": \"you have\",\n            \"colour\": \"color\",\n            \"centre\": \"center\",\n            \"favourite\": \"favorite\",\n            \"travelling\": \"traveling\",\n            \"counselling\": \"counseling\",\n            \"theatre\": \"theater\",\n            \"cancelled\": \"canceled\",\n            \"labour\": \"labor\",\n            \"organisation\": \"organization\",\n            \"wwii\": \"world war 2\",\n            \"citicise\": \"criticize\",\n            \"youtu \": \"youtube \",\n            \"qoura\": \"quora\",\n            \"sallary\": \"salary\",\n            \"whta\": \"what\",\n            \"narcisist\": \"narcissist\",\n            \"howdo\": \"how do\",\n            \"whatare\": \"what are\",\n            \"howcan\": \"how can\",\n            \"howmuch\": \"how much\",\n            \"howmany\": \"how many\",\n            \"whydo\": \"why do\",\n            \"doi\": \"do i\",\n            \"thebest\": \"the best\",\n            \"howdoes\": \"how does\",\n            \"mastrubation\": \"masturbation\",\n            \"mastrubate\": \"masturbate\",\n            \"mastrubating\": \"masturbating\",\n            \"pennis\": \"penis\",\n            \"etherium\": \"ethereum\",\n            \"narcissit\": \"narcissist\",\n            \"bigdata\": \"big data\",\n            \"2k17\": \"2017\",\n            \"2k18\": \"2018\",\n            \"qouta\": \"quota\",\n            \"exboyfriend\": \"ex boyfriend\",\n            \"airhostess\": \"air hostess\",\n            \"whst\": \"what\",\n            \"watsapp\": \"whatsapp\",\n            \"demonitisation\": \"demonetization\",\n            \"demonitization\": \"demonetization\",\n            \"demonetisation\": \"demonetization\",\n        }\n        super().__init__(rule)\n\n\nregister_preprocessor('lower')(cylower)\nregister_preprocessor('punct')(PunctSpacer())\nregister_preprocessor('unidecode')(unidecode)\nregister_preprocessor('unidecode_weak')(unidecode_weak)\nregister_preprocessor('number')(NumberReplacer())\nregister_preprocessor('number+underscore')(\n    NumberReplacer(with_underscore=True))\nregister_preprocessor('misspell')(MisspellReplacer())\nregister_preprocessor('keras')(KerasFilterReplacer())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_pretrained_vectors(names, token2id, test=False):\n    assert isinstance(names, list)\n    with Pool(processes=len(names)) as pool:\n        f = partial(load_pretrained_vector, token2id=token2id, test=test)\n        vectors = pool.map(f, names)\n    return dict([(n, v) for n, v in zip(names, vectors)])\n\n\ndef load_pretrained_vector(name, token2id, test=False):\n    loader = dict(\n        gnews=GNewsPretrainedVector,\n        wnews=WNewsPretrainedVector,\n        paragram=ParagramPretrainedVector,\n        glove=GlovePretrainedVector,\n    )\n    return loader[name].load(token2id, test)\n\n\nclass BasePretrainedVector(object):\n\n    @classmethod\n    def load(cls, token2id, test=False, limit=None):\n        embed_shape = (len(token2id), 300)\n        freqs = np.zeros((len(token2id)), dtype='f')\n\n        if test:\n            np.random.seed(0)\n            vectors = np.random.normal(0, 1, embed_shape)\n            vectors[0] = 0\n            vectors[len(token2id) // 2:] = 0\n        else:\n            vectors = np.zeros(embed_shape, dtype='f')\n            path = f'{os.environ[\"DATADIR\"]}/{cls.path}'\n            for i, o in enumerate(\n                    open(path, encoding=\"utf8\", errors='ignore')):\n                token, *vector = o.split(' ')\n                token = str.lower(token)\n                if token not in token2id or len(o) <= 100:\n                    continue\n                if limit is not None and i > limit:\n                    break\n                freqs[token2id[token]] += 1\n                vectors[token2id[token]] += np.array(vector, 'f')\n\n        vectors[freqs != 0] /= freqs[freqs != 0][:, None]\n        vec = KeyedVectors(300)\n        vec.add(list(token2id.keys()), vectors, replace=True)\n\n        return vec\n\n\nclass GNewsPretrainedVector(object):\n\n    name = 'GoogleNews-vectors-negative300'\n    path = f'embeddings/{name}/{name}.bin'\n\n    @classmethod\n    def load(cls, tokens, limit=None):\n        raise NotImplementedError\n        path = f'{os.environ[\"DATADIR\"]}/{cls.path}'\n        return KeyedVectors.load_word2vec_format(\n            path, binary=True, limit=limit)\n\n\nclass WNewsPretrainedVector(BasePretrainedVector):\n\n    name = 'wiki-news-300d-1M'\n    path = f'embeddings/{name}/{name}.vec'\n\n\nclass ParagramPretrainedVector(BasePretrainedVector):\n\n    name = 'paragram_300_sl999'\n    path = f'embeddings/{name}/{name}.txt'\n\n\nclass GlovePretrainedVector(BasePretrainedVector):\n\n    name = 'glove.840B.300d'\n    path = f'embeddings/{name}/{name}.txt'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@register_word_embedding_features('pretrained')\nclass PretrainedVectorFeaturizer(object):\n\n    def __init__(self, config, vocab):\n        self.config = config\n        self.vocab = vocab\n\n    @classmethod\n    def add_args(self, parser):\n        pass\n\n    def __call__(self, features, datasets):\n        # Nothing to do\n        return features\n\n\nclass Any2VecFeaturizer(object):\n\n    def __init__(self, config, vocab):\n        self.config = config\n        self.vocab = vocab\n\n    def build_fillvalue(self, mode, initialW):\n        n_embed = initialW.shape[1]\n        n_fill = initialW[self.vocab.unk].shape\n        assert mode in {'zeros', 'mean', 'noise'}\n        if mode == 'zeros':\n            return np.zeros(n_embed, 'f')\n        elif mode == 'mean':\n            return initialW.mean(axis=0)\n        elif mode == 'noise':\n            mean, std = initialW.mean(), initialW.std()\n            return np.random.normal(mean, std, (n_fill, n_embed))\n\n    def __call__(self, features, datasets):\n        tokens = np.concatenate([d.tokens for d in datasets])\n        model = self.build_model()\n        model.build_vocab_from_freq(self.vocab.word_freq)\n        initialW = features.copy()\n        initialW[self.vocab.unk] = self.build_fillvalue(\n            self.config.finetune_word2vec_init_unk, initialW)\n        idxmap = np.array(\n            [self.vocab.token2id[w] for w in model.wv.index2entity])\n        model = self.initialize(model, initialW, idxmap)\n        model.train(tokens, total_examples=len(tokens), epochs=model.epochs)\n        finetunedW = np.zeros((initialW.shape), 'f')\n        for i, word in enumerate(self.vocab.token2id):\n            if word in model.wv:\n                finetunedW[i] = model.wv.get_vector(word)\n        return finetunedW\n\n\n@register_word_embedding_features('word2vec')\nclass Word2VecFeaturizer(Any2VecFeaturizer):\n\n    @classmethod\n    def add_args(self, parser):\n        parser.add_argument('--finetune-word2vec-init-unk', type=str,\n                            choices=['zeros', 'mean', 'noise'])\n        parser.add_argument('--finetune-word2vec-mincount', type=int)\n        parser.add_argument('--finetune-word2vec-workers', type=int)\n        parser.add_argument('--finetune-word2vec-iter', type=int)\n        parser.add_argument('--finetune-word2vec-size', type=int)\n        parser.add_argument('--finetune-word2vec-window', type=int, default=5)\n        parser.add_argument('--finetune-word2vec-sorted-vocab', type=int,\n                            default=0)\n        parser.add_argument('--finetune-word2vec-sg', type=int, choices=[0, 1])\n\n    def build_model(self):\n        model = Word2Vec(\n            min_count=self.config.finetune_word2vec_mincount,\n            workers=self.config.finetune_word2vec_workers,\n            iter=self.config.finetune_word2vec_iter,\n            size=self.config.finetune_word2vec_size,\n            window=self.config.finetune_word2vec_window,\n            sg=self.config.finetune_word2vec_sg,\n        )\n        return model\n\n    def initialize(self, model, initialW, idxmap):\n        model.wv.vectors[:] = initialW[idxmap]\n        model.trainables.syn1neg[:] = initialW[idxmap]\n        return model\n\n\n@register_word_embedding_features('fasttext')\nclass FastTextFeaturizer(Any2VecFeaturizer):\n\n    @classmethod\n    def add_args(self, parser):\n        parser.add_argument('--finetune-fasttext-init-unk', type=str,\n                            choices=['zeros', 'mean', 'noise'])\n        parser.add_argument('--finetune-fasttext-mincount', type=int)\n        parser.add_argument('--finetune-fasttext-workers', type=int)\n        parser.add_argument('--finetune-fasttext-iter', type=int)\n        parser.add_argument('--finetune-fasttext-size', type=int)\n        parser.add_argument('--finetune-fasttext-sg', type=int, choices=[0, 1])\n        parser.add_argument('--finetune-fasttext-min_n', type=int)\n        parser.add_argument('--finetune-fasttext-max_n', type=int)\n\n    def build_model(self):\n        model = FastText(\n            min_count=self.config.finetune_fasttext_mincount,\n            workers=self.config.finetune_fasttext_workers,\n            iter=self.config.finetune_fasttext_iter,\n            size=self.config.finetune_fasttext_size,\n            sg=self.config.finetune_fasttext_sg,\n            min_n=self.config.finetune_fasttext_min_n,\n            max_n=self.config.finetune_fasttext_max_n,\n        )\n        return model\n\n    def initialize(self, model, initialW, idxmap):\n        model.wv.vectors[:] = initialW[idxmap]\n        model.wv.vectors_vocab[:] = initialW[idxmap]\n        model.trainables.syn1neg[:] = initialW[idxmap]\n        return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@register_word_extra_features('idf')\nclass IDFWordFeaturizer(object):\n\n    def __call__(self, vocab):\n        dfs = np.array(list(vocab.word_freq.values()))\n        dfs[0] = vocab.n_documents\n        features = np.log(vocab.n_documents / dfs)\n        features = features[:, None]\n        return features\n\n\n# @register_word_extra_features('unk')\n# class UnkWordFeaturizer(object):\n\n#     def __call__(self, vocab):\n#         features = vocab.unk.astype('f')\n#         features[0] = 0\n#         features = features[:, None]\n#         return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@register_sentence_extra_features('char')\nclass CharacterStatisticsFeaturizer(object):\n\n    n_dims = 3\n\n    def __call__(self, sentence):\n        feature = {}\n        feature['n_chars'] = len(sentence)\n        feature['n_caps'] = sum(1 for char in sentence if char.isupper())\n        feature['caps_rate'] = feature['n_caps'] / feature['n_chars']\n        features = np.array(list(feature.values()))\n        return features\n\n\n@register_sentence_extra_features('word')\nclass WordStatisticsFeaturizer(object):\n\n    n_dims = 3\n\n    def __call__(self, sentence):\n        feature = {}\n        tokens = sentence.split()\n        feature['n_words'] = len(tokens)\n        feature['unique_words'] = len(set(tokens))\n        feature['unique_rate'] = feature['unique_words'] / feature['n_words']\n        features = np.array(list(feature.values()))\n        return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%cython\ncpdef list cysplit(str x):\n    return x.split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"register_tokenizer('space')(cysplit)\nregister_tokenizer('word_tokenize')(nltk.word_tokenize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextNormalizerWrapper(object):\n\n    registry = NORMALIZER_REGISTRY\n    default_config = None\n\n    def __init__(self, config):\n        self.normalizers = [self.registry[n] for n in config.normalizers]\n\n    @classmethod\n    def add_args(cls, parser):\n        assert isinstance(cls.default_config, dict)\n        parser.add_argument(\n            '--normalizers', nargs='+', choices=cls.registry)\n        parser.set_defaults(**cls.default_config)\n\n    def __call__(self, x):\n        for normalizer in self.normalizers:\n            x = normalizer(x)\n        return x\n\n    \nclass TextTokenizerWrapper(object):\n\n    registry = TOKENIZER_REGISTRY\n    default_config = None\n\n    def __init__(self, config):\n        self.tokenizer = self.registry[config.tokenizer]\n\n    @classmethod\n    def add_args(cls, parser):\n        assert isinstance(cls.default_config, dict)\n        parser.add_argument('--tokenizer', choices=cls.registry)\n        parser.set_defaults(**cls.default_config)\n\n    def __call__(self, x):\n        return self.tokenizer(x)\n\n    \nclass WordEmbeddingFeaturizerWrapper(object):\n\n    registry = WORD_EMBEDDING_FEATURIZER_REGISTRY\n    default_config = None\n    default_extra_config = None\n\n    def __init__(self, config, vocab):\n        self.config = config\n        self.vocab = vocab\n        self.featurizers = {\n            k: self.registry[k](config, vocab)\n            for k in config.word_embedding_features}\n\n    @classmethod\n    def add_args(cls, parser):\n        assert isinstance(cls.default_config, dict)\n        parser.add_argument(\n            '--use-pretrained-vectors', nargs='+',\n            choices=['glove', 'paragram', 'wnews', 'gnews'])\n        parser.add_argument(\n            '--word-embedding-features', nargs='+', choices=cls.registry)\n        parser.set_defaults(**cls.default_config)\n\n    @classmethod\n    def add_extra_args(cls, parser, config):\n        assert isinstance(cls.default_extra_config, dict)\n        for featurizer in config.word_embedding_features:\n            cls.registry[featurizer].add_args(parser)\n        parser.set_defaults(**cls.default_extra_config)\n\n    def __call__(self, features, datasets):\n        return {k: feat(features, datasets)\n                for k, feat in self.featurizers.items()}\n\n\nclass WordExtraFeaturizerWrapper(object):\n\n    registry = WORD_EXTRA_FEATURIZER_REGISTRY\n    default_config = None\n\n    def __init__(self, config, vocab):\n        self.config = config\n        self.vocab = vocab\n        self.featurizers = {\n            k: self.registry[k]() for k in config.word_extra_features}\n\n    @classmethod\n    def add_args(cls, parser):\n        parser.add_argument(\n            '--word-extra-features', nargs='+', choices=cls.registry)\n        parser.set_defaults(**cls.default_config)\n\n    def __call__(self, vocab):\n        empty = np.empty([len(vocab), 0])\n        return np.concatenate([empty, *[\n            f(vocab) for f in self.featurizers.values()]], axis=1)\n\n\nclass SentenceExtraFeaturizerWrapper(object):\n\n    registry = SENTENCE_EXTRA_FEATURIZER_REGISTRY\n    default_config = None\n\n    def __init__(self, config):\n        self.config = config\n        self.featurizers = {\n            k: self.registry[k]() for k in config.sentence_extra_features}\n        self.n_dims = sum(list(f.n_dims for f in self.featurizers.values()))\n\n    @classmethod\n    def add_args(cls, parser):\n        parser.add_argument(\n            '--sentence-extra-features', nargs='+', choices=cls.registry)\n        parser.set_defaults(**cls.default_config)\n\n    def __call__(self, sentence):\n        empty = np.empty((0,))\n        return np.concatenate([empty, *[\n            f(sentence) for f in self.featurizers.values()]], axis=0)\n\n    def fit_standardize(self, features):\n        assert features.ndim == 2\n        self.mean = features.mean(axis=0)\n        self.std = features.std(axis=0)\n        self.std = np.where(self.std != 0, self.std, 1)\n        return (features - self.mean) / self.std\n\n    def standardize(self, features):\n        assert hasattr(self, 'mean'), hasattr(self, 'std')\n        return (features - self.mean) / self.std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class WordbasedPreprocessor():\n\n    def tokenize(self, datasets, normalizer, tokenizer):\n        tokenize = Pipeline(normalizer, tokenizer)\n        apply_tokenize = ApplyNdArray(tokenize, processes=2, dtype=object)\n        tokens = [apply_tokenize(d.df.question_text.values) for d in datasets]\n        return tokens\n\n    def build_vocab(self, datasets, config):\n        train_dataset, test_dataset, submit_dataset = datasets\n        vocab = WordVocab(mincount=config.vocab_mincount)\n        vocab.add_documents(train_dataset.positives.tokens, 'train-pos')\n        vocab.add_documents(train_dataset.negatives.tokens, 'train-neg')\n        vocab.add_documents(test_dataset.positives.tokens, 'test-pos')\n        vocab.add_documents(test_dataset.negatives.tokens, 'test-neg')\n        vocab.add_documents(submit_dataset.df.tokens, 'submit')\n        vocab.build()\n        return vocab\n\n    def build_tokenids(self, datasets, vocab, config):\n        token2id = lambda xs: pad_sequence(  # NOQA\n            [vocab.token2id[x] for x in xs], config.maxlen)\n        apply_token2id = ApplyNdArray(\n            token2id, processes=1, dtype='i', dims=(config.maxlen,))\n        tokenids = [apply_token2id(d.df.tokens.values) for d in datasets]\n        return tokenids\n\n    def build_sentence_features(self, datasets, sentence_extra_featurizer):\n        train_dataset, test_dataset, submit_dataset = datasets\n        apply_featurize = ApplyNdArray(\n            sentence_extra_featurizer, processes=1, dtype='f',\n            dims=(sentence_extra_featurizer.n_dims,))\n        _X2 = [apply_featurize(d.df.question_text.values) for d in datasets]\n        _train_X2, _test_X2, _submit_X2 = _X2\n        train_X2 = sentence_extra_featurizer.fit_standardize(_train_X2)\n        test_X2 = sentence_extra_featurizer.standardize(_test_X2)\n        submit_X2 = sentence_extra_featurizer.standardize(_submit_X2)\n        return train_X2, test_X2, submit_X2\n\n    def build_embedding_matrices(self, datasets, word_embedding_featurizer,\n                                 vocab, pretrained_vectors):\n        pretrained_vectors_merged = np.stack(\n            [wv.vectors for wv in pretrained_vectors.values()]).mean(axis=0)\n        vocab.unk = (pretrained_vectors_merged == 0).all(axis=1)\n        vocab.known = ~vocab.unk\n        embedding_matrices = word_embedding_featurizer(\n            pretrained_vectors_merged, datasets)\n        return embedding_matrices\n\n    def build_word_features(self, word_embedding_featurizer,\n                            embedding_matrices, word_extra_features):\n        embedding = np.stack(list(embedding_matrices.values()))\n        embedding = embedding.mean(axis=0)\n        word_features = np.concatenate(\n            [embedding, word_extra_features], axis=1)\n        return word_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class RNNEncoderBase(nn.Module):\n\n    def __init__(self, config, modules, in_size):\n        super().__init__()\n        rnns = []\n        input_size = in_size\n        for module in modules:\n            rnn = module(\n                input_size=input_size,\n                hidden_size=config.encoder_n_hidden,\n                bidirectional=config.encoder_bidirectional,\n                batch_first=True,\n            )\n            n_direction = int(config.encoder_bidirectional) + 1\n            input_size = n_direction * config.encoder_n_hidden\n            rnns.append(rnn)\n        self.rnns = nn.ModuleList(rnns)\n        self.out_size = n_direction * config.encoder_n_hidden\n\n    @classmethod\n    def add_args(self, parser):\n        parser.add_argument('--encoder-bidirectional', type=bool, default=True)\n        parser.add_argument('--encoder-dropout', type=float, default=0.)\n        parser.add_argument('--encoder-n-hidden', type=int)\n        parser.add_argument('--encoder-n-layers', type=int)\n        parser.add_argument('--encoder-aggregator', type=str,\n                            choices=AGGREGATOR_REGISTRY)\n\n    def forward(self, input, mask):\n        h = input\n        for rnn in self.rnns:\n            h, _ = rnn(h)\n        return h\n\n\n@register_encoder('lstm')\nclass LSTMEncoder(RNNEncoderBase):\n\n    def __init__(self, config, in_size):\n        modules = [nn.LSTM] * config.encoder_n_layers\n        super().__init__(config, modules, in_size)\n\n\n@register_encoder('gru')\nclass GRUEncoder(RNNEncoderBase):\n\n    def __init__(self, config, in_size):\n        assert config.encoder_n_layers > 1\n        modules = [nn.GRU] * config.encoder_n_layers\n        super().__init__(config, modules, in_size)\n\n\n@register_encoder('lstmgru')\nclass LSTMGRUEncoder(RNNEncoderBase):\n\n    def __init__(self, config, in_size):\n        assert config.encoder_n_layers > 1\n        modules = [nn.LSTM] * (config.encoder_n_layers - 1) + [nn.GRU]\n        super().__init__(config, modules, in_size)\n\n\n@register_encoder('grulstm')\nclass GRULSTMEncoder(RNNEncoderBase):\n\n    def __init__(self, config, in_size):\n        assert config.encoder_n_layers > 1\n        modules = [nn.GRU] * (config.encoder_n_layers - 1) + [nn.LSTM]\n        super().__init__(config, modules, in_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@register_aggregator('max')\nclass MaxPoolingAggregator(nn.Module):\n\n    def __call__(self, hs, mask):\n        if mask is not None:\n            hs = hs.masked_fill(~mask.unsqueeze(2), -np.inf)\n        h = hs.max(dim=1)[0]\n        return h\n\n\n@register_aggregator('sum')\nclass SumPoolingAggregator(nn.Module):\n\n    def __call__(self, hs, mask):\n        if mask is not None:\n            hs = hs.masked_fill(~mask.unsqueeze(2), 0)\n        h = hs.sum(dim=1)\n        return h\n\n\n@register_aggregator('avg')\nclass AvgPoolingAggregator(nn.Module):\n\n    def __call__(self, hs, mask):\n        if mask is not None:\n            hs = hs.masked_fill(~mask.unsqueeze(2), 0)\n        h = hs.sum(dim=1)\n        maxlen = mask.sum(dim=1)\n        h /= maxlen[:, None].type(torch.float)\n        return h","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BaseEnsembler(metaclass=ABCMeta):\n\n    def __init__(self, config, models, results):\n        super().__init__()\n        self.config = config\n        self.models = models\n        self.results = results\n\n    @abstractmethod\n    def fit(self, X, t, test_size=0.1):\n        pass\n\n    @abstractmethod\n    def predict_proba(self, X, X2):\n        pass\n\n    def predict(self, X, X2):\n        y = self.predict_proba(X, X2)\n        return (y > self.threshold).astype('i')\n\n    \nclass AverageEnsembler(BaseEnsembler):\n\n    def __init__(self, config, models, results):\n        self.config = config\n        self.models = models\n        self.results = results\n        self.device = config.device\n        self.batchsize_train = config.batchsize\n        self.batchsize_valid = config.batchsize_valid\n        self.threshold_cv = np.array(\n            [m.threshold for m in models]).mean()\n        self.threshold = self.threshold_cv\n\n    def fit(self, X, X2, t, test_size=0.1):\n        # Nothing to do\n        pass\n\n    def predict_proba(self, X, X2):\n        pred_X = X.to(self.device)\n        pred_X2 = X2.to(self.device)\n        dataset = torch.utils.data.TensorDataset(pred_X, pred_X2)\n        iterator = DataLoader(\n            dataset, batch_size=self.batchsize_valid, shuffle=False)\n        ys = defaultdict(list)\n        for batch in tqdm(iterator, desc='submit', leave=False):\n            for i, model in enumerate(self.models):\n                model.eval()\n                ys[i].append(model.predict_proba(*batch))\n        ys = np.concatenate(\n            [np.concatenate(_ys) for _ys in ys.values()], axis=1)\n        y = ys.mean(axis=1, keepdims=True)\n        return y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BinaryClassifier(nn.Module):\n\n    default_config = None\n\n    def __init__(self, embedding, encoder, aggregator, mlp, out, lossfunc):\n        super().__init__()\n        self.embedding = embedding\n        self.encoder = encoder\n        self.aggregator = aggregator\n        self.mlp = mlp\n        self.out = out\n        self.lossfunc = lossfunc\n\n    def calc_loss(self, X, X2, t, W=None):\n        y = self.forward(X, X2)\n        loss = self.lossfunc(y, t)\n        output = dict(\n            y=torch.sigmoid(y).cpu().detach().numpy(),\n            t=t.cpu().detach().numpy(),\n            loss=loss.cpu().detach().numpy(),\n        )\n        return loss, output\n\n    def to_device(self, device):\n        self.device = device\n        self.to(device)\n        return self\n\n    def forward(self, X, X2):\n        h = self.predict_features(X, X2)\n        out = self.out(h)\n        return out\n\n    def predict_proba(self, X, X2):\n        y = self.forward(X, X2)\n        proba = torch.sigmoid(y).cpu().detach().numpy()\n        return proba\n\n    def predict_features(self, X, X2):\n        mask = X != 0\n        maxlen = (mask == 1).any(dim=0).sum()\n        X = X[:, :maxlen]\n        mask = mask[:, :maxlen]\n\n        h = self.embedding(X)\n        h = self.encoder(h, mask)\n        h = self.aggregator(h, mask)\n        h = self.mlp(h, X2)\n        return h","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NNModuleWrapperBase(nn.Module, metaclass=ABCMeta):\n\n    @abstractmethod\n    def add_args(cls, parser):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def add_extra_args(cls, parser):\n        raise NotImplementedError()\n\n        \nclass EmbeddingWrapper(NNModuleWrapperBase):\n\n    default_config = None\n\n    def __init__(self, config, embedding_matrix):\n        super().__init__()\n        self.config = config\n        self.module = nn.Embedding.from_pretrained(\n            torch.Tensor(embedding_matrix), freeze=True)\n        if self.config.embedding_dropout1d > 0:\n            self.dropout1d = nn.Dropout(config.embedding_dropout1d)\n        if self.config.embedding_dropout2d > 0:\n            self.dropout2d = nn.Dropout2d(config.embedding_dropout2d)\n        if self.config.embedding_spatial_dropout > 0:\n            self.spatial_dropout = nn.Dropout2d(\n                config.embedding_spatial_dropout)\n        self.out_size = embedding_matrix.shape[1]\n\n    @classmethod\n    def add_args(cls, parser):\n        assert isinstance(cls.default_config, dict)\n        parser.add_argument('--embedding-dropout1d', type=float, default=0.)\n        parser.add_argument('--embedding-dropout2d', type=float, default=0.)\n        parser.add_argument('--embedding-spatial-dropout',\n                            type=float, default=0.)\n        parser.set_defaults(**cls.default_config)\n\n    @classmethod\n    def add_extra_args(cls, parser, config):\n        pass\n\n    def forward(self, X):\n        h = self.module(X)\n        if self.config.embedding_dropout1d > 0:\n            h = self.dropout1d(h)\n        if self.config.embedding_dropout2d > 0:\n            h = self.dropout2d(h)\n        if self.config.embedding_spatial_dropout > 0:\n            h = h.permute(0, 2, 1)\n            h = self.spatial_dropout(h)\n            h = h.permute(0, 2, 1)\n        return h\n\n    \nclass EncoderWrapper(nn.Module):\n\n    registry = ENCODER_REGISTRY\n\n    def __init__(self, config, in_size):\n        super().__init__()\n        self.config = config\n        self.module = self.registry[config.encoder](config, in_size)\n        self.out_size = self.module.out_size\n\n    @classmethod\n    def add_args(cls, parser):\n        assert isinstance(cls.default_config, dict)\n        parser.add_argument(\n            '--encoder', choices=cls.registry)\n        parser.set_defaults(**cls.default_config)\n\n    @classmethod\n    def add_extra_args(cls, parser, config):\n        assert isinstance(cls.default_extra_config, dict)\n        cls.registry[config.encoder].add_args(parser)\n        parser.set_defaults(**cls.default_extra_config)\n\n    def forward(self, X, mask):\n        h = self.module(X, mask)\n        return h\n\n    \nclass AggregatorWrapper(NNModuleWrapperBase):\n\n    default_config = None\n    registry = AGGREGATOR_REGISTRY\n\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.module = self.registry[config.aggregator]()\n\n    @classmethod\n    def add_args(cls, parser):\n        assert isinstance(cls.default_config, dict)\n        parser.add_argument('--aggregator',\n                            choices=cls.registry)\n        parser.set_defaults(**cls.default_config)\n\n    @classmethod\n    def add_extra_args(cls, parser, config):\n        pass\n\n    def forward(self, X, mask):\n        h = self.module(X, mask)\n        return h\n\n    \nclass MLPWrapper(NNModuleWrapperBase):\n\n    default_config = None\n\n    def __init__(self, config, in_size):\n        super().__init__()\n        self.in_size = in_size\n        self.config = config\n        assert isinstance(config.mlp_n_hiddens, list)\n        layers = []\n        if config.mlp_bn0:\n            layers.append(nn.BatchNorm1d(in_size))\n        if config.mlp_dropout0 > 0:\n            layers.append(nn.Dropout(config.mlp_dropout0))\n        for n_hidden in config.mlp_n_hiddens:\n            layers.append(nn.Linear(in_size, n_hidden))\n            if config.mlp_actfun is not None:\n                layers.append(config.mlp_actfun)\n            if config.mlp_bn:\n                layers.append(nn.BatchNorm1d(n_hidden))\n            if config.mlp_dropout > 0:\n                layers.append(nn.Dropout(config.mlp_dropout))\n            in_size = n_hidden\n        self.layers = nn.Sequential(*layers)\n\n    @classmethod\n    def add_args(cls, parser):\n        assert isinstance(cls.default_config, dict)\n        parser.add_argument('--mlp-n-hiddens', type=list)\n        parser.add_argument('--mlp-bn', type=bool)\n        parser.add_argument('--mlp-bn0', type=bool)\n        parser.add_argument('--mlp-dropout', type=float, default=0.)\n        parser.add_argument('--mlp-dropout0', type=float, default=0.)\n        parser.add_argument('--mlp-actfun', default=0.)\n        parser.set_defaults(**cls.default_config)\n\n    @classmethod\n    def add_extra_args(cls, parser, config):\n        pass\n\n    def forward(self, X, X2):\n        h = X\n        if X.shape[1] + X2.shape[1] == self.in_size:\n            h = torch.cat([h, X2], dim=1)\n        h = self.layers(h)\n        return h","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =======  Experiment configuration  =======\n\nclass ExperimentConfigBuilderPresets(ExperimentConfigBuilderBase):\n\n    default_config = dict(\n        maxlen=72,\n        vocab_mincount=5,\n        scale_batchsize=[],\n        validate_from=2,\n    )\n\n\n# =======  Preprocessing modules  =======\n\nclass TextNormalizerPresets(TextNormalizerWrapper):\n\n    default_config = dict(\n        normalizers=[\n            'lower',\n            'misspell',\n            'punct',\n            'number+underscore'\n        ]\n    )\n\n\nclass TextTokenizerPresets(TextTokenizerWrapper):\n\n    default_config = dict(\n        tokenizer='space'\n    )\n\n\nclass WordEmbeddingFeaturizerPresets(WordEmbeddingFeaturizerWrapper):\n\n    default_config = dict(\n        use_pretrained_vectors=['glove', 'paragram'],\n        word_embedding_features=['pretrained', 'word2vec'],\n    )\n    default_extra_config = dict(\n        finetune_word2vec_init_unk='zeros',\n        finetune_word2vec_mincount=1,\n        finetune_word2vec_workers=1,\n        finetune_word2vec_iter=5,\n        finetune_word2vec_size=300,\n        finetune_word2vec_sg=0,\n        finetune_word2vec_sorted_vocab=0,\n    )\n\n\nclass WordExtraFeaturizerPresets(WordExtraFeaturizerWrapper):\n\n    default_config = dict(\n        word_extra_features=[],\n    )\n\n\nclass SentenceExtraFeaturizerPresets(SentenceExtraFeaturizerWrapper):\n\n    default_config = dict(\n        sentence_extra_features=[],\n    )\n\n\nclass PreprocessorPresets(WordbasedPreprocessor):\n\n    def build_word_features(self, word_embedding_featurizer,\n                            embedding_matrices, word_extra_features):\n        embedding = np.stack(list(embedding_matrices.values()))\n\n        # Add noise\n        unk = (embedding[0] == 0).all(axis=1)\n        mean, std = embedding[0, ~unk].mean(), embedding[0, ~unk].std()\n        unk_and_hfq = unk & word_embedding_featurizer.vocab.hfq\n        noise = np.random.normal(\n            mean, std, (unk_and_hfq.sum(), embedding[0].shape[1]))\n        embedding[0, unk_and_hfq] = noise\n        embedding[0, 0] = 0\n\n        embedding = embedding.mean(axis=0)\n        word_features = np.concatenate(\n            [embedding, word_extra_features], axis=1)\n        return word_features\n\n\n# =======  Training modules  =======\n\nclass EmbeddingPresets(EmbeddingWrapper):\n\n    default_config = dict(\n        embedding_dropout1d=0.2,\n    )\n\n\nclass EncoderPresets(EncoderWrapper):\n\n    default_config = dict(\n        encoder='lstm',\n    )\n    default_extra_config = dict(\n        encoder_bidirectional=True,\n        encoder_dropout=0.,\n        encoder_n_layers=2,\n        encoder_n_hidden=128,\n    )\n\n\nclass AggregatorPresets(AggregatorWrapper):\n\n    default_config = dict(\n        aggregator='max',\n    )\n\n\nclass MLPPresets(MLPWrapper):\n\n    default_config = dict(\n        mlp_n_hiddens=[128, 128],\n        mlp_bn0=False,\n        mlp_dropout0=0.,\n        mlp_bn=True,\n        mlp_actfun=nn.ReLU(True),\n    )\n\n\nclass EnsemblerPresets(AverageEnsembler):\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def classification_metrics(ys, ts):\n    scores = {}\n\n    if len(np.unique(ts)) > 1:\n        # Search optimal threshold\n        precs, recs, thresholds = metrics.precision_recall_curve(ts, ys)\n        thresholds = np.append(thresholds, 1.001)\n        idx = (precs != 0) * (recs != 0)\n        precs, recs, thresholds = precs[idx], recs[idx], thresholds[idx]\n        fbetas = 2 / (1 / precs + 1 / recs)\n        best_idx = np.argmax(fbetas)\n        threshold = thresholds[best_idx]\n        prec = precs[best_idx]\n        rec = recs[best_idx]\n        fbeta = fbetas[best_idx]\n\n        scores['ap'] = metrics.average_precision_score(ts, ys)\n        scores['rocauc'] = metrics.roc_auc_score(ts, ys)\n        scores['threshold'] = threshold\n        scores['prec'] = prec\n        scores['rec'] = rec\n        scores['fbeta'] = fbeta\n\n    return scores\n\n\nclass ClassificationResult(object):\n\n    def __init__(self, name, outdir=None, postfix=None, main_metrics='fbeta'):\n        self.initialize()\n        self.name = name\n        self.postfix = postfix\n        self.outdir = outdir\n        self.summary = None\n        self.main_metrics = main_metrics\n        self.n_trained = 0\n\n    def initialize(self):\n        self.losses = []\n        self.ys = []\n        self.ts = []\n\n    def add_record(self, loss, y, t):\n        self.losses.append(loss)\n        self.ys.append(y)\n        self.ts.append(t)\n        self.n_trained += len(y)\n\n    def calc_score(self, epoch):\n        loss = np.array(self.losses).mean()\n        self.ys, self.ts = np.concatenate(self.ys), np.concatenate(self.ts)\n        score = classification_metrics(self.ys, self.ts)\n        summary = dict(name=self.name, loss=loss, **score)\n        if len(score) > 0:\n            if self.summary is None:\n                self.summary = pd.DataFrame([summary], index=[epoch])\n                self.summary.index.name = 'epoch'\n            else:\n                self.summary.loc[epoch] = summary\n        if self.best_epoch == epoch:\n            self.best_ys = self.ys\n            self.best_ts = self.ts\n        self.initialize()\n\n    def get_dict(self):\n        loss, fbeta, epoch = 0, 0, 0\n        if self.summary is not None:\n            row = self.summary.iloc[-1]\n            epoch = row.name\n            loss = row.loss\n            fbeta = row.fbeta\n        return {\n            'epoch': epoch,\n            'loss': loss,\n            'fbeta': fbeta,\n        }\n\n    @property\n    def fbeta(self):\n        if self.summary is None:\n            return 0\n        else:\n            return self.summary.fbeta[-1]\n\n    @property\n    def best_fbeta(self):\n        return self.summary[self.main_metrics].max()\n\n    @property\n    def best_epoch(self):\n        return self.summary[self.main_metrics].idxmax()\n\n    @property\n    def best_threshold(self):\n        idx = self.summary[self.main_metrics].idxmax()\n        return self.summary['threshold'][idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%cython\nimport numpy as np\ncimport numpy as np\nfrom multiprocessing import Pool\n\n\ncdef class ApplyNdArray:\n    cdef func\n    cdef dtype\n    cdef dims\n    cdef int processes\n\n    def __init__(self, func, processes=1, dtype=object, dims=None):\n        self.func = func\n        self.processes = processes\n        self.dtype = dtype\n        self.dims = dims\n\n    def __call__(self, arr):\n        if self.processes == 1:\n            return self.apply(arr)\n        else:\n            return self.apply_parallel(arr)\n\n    cpdef apply(self, arr):\n        cdef int i\n        cdef int n = len(arr)\n        if self.dims is not None:\n            shape = (n, *self.dims)\n        else:\n            shape = n\n        cdef res = np.empty(shape, dtype=self.dtype)\n        for i in range(n):\n            res[i] = self.func(arr[i])\n        return res\n\n    cpdef apply_parallel(self, arr):\n        cdef list arrs = np.array_split(arr, self.processes)\n        with Pool(processes=self.processes) as pool:\n            outputs = pool.map(self.apply, arrs)\n        return np.concatenate(outputs, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_module(filename):\n    assert isinstance(filename, Path)\n    name = filename.stem\n    spec = importlib.util.spec_from_file_location(name, filename)\n    mod = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(mod)\n    sys.modules[mod.__name__] = mod\n    return mod\n\n\ndef rmtree_after_confirmation(path, force=False):\n    if Path(path).exists():\n        if not force and not prompter.yesno('Overwrite %s?' % path):\n            sys.exit(0)\n        else:\n            shutil.rmtree(path)\n\n\ndef pad_sequence(xs, length, padding_value=0):\n    assert isinstance(xs, list)\n    n_padding = length - len(xs)\n    return np.array(xs + [padding_value] * n_padding, 'i')[:length]\n\n\ndef set_seed(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\nclass Pipeline(object):\n\n    def __init__(self, *modules):\n        self.modules = modules\n\n    def __call__(self, x):\n        for module in self.modules:\n            x = module(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\n\n\n\n# =======  Preprocessing modules  =======\n\nclass TextNormalizer(TextNormalizerPresets):\n    pass\n\n\nclass TextTokenizer(TextTokenizerPresets):\n    pass\n\n\nclass WordEmbeddingFeaturizer(WordEmbeddingFeaturizerPresets):\n    pass\n\n\nclass WordExtraFeaturizer(WordExtraFeaturizerPresets):\n\n    default_config = dict(\n        word_extra_features=['idf'],\n    )\n\n\nclass SentenceExtraFeaturizer(SentenceExtraFeaturizerPresets):\n\n    default_config = dict(\n        sentence_extra_features=['char', 'word'],\n    )\n\n\nclass Preprocessor(PreprocessorPresets):\n\n    embedding_sampling = 400\n\n    def build_word_features(self, word_embedding_featurizer,\n                            embedding_matrices, word_extra_features):\n        embedding = np.stack(list(embedding_matrices.values()))\n\n        # Concat embedding\n        embedding = np.concatenate(embedding, axis=1)\n        vocab = word_embedding_featurizer.vocab\n        embedding[vocab.lfq & vocab.unk] = 0\n\n        # Embedding random sampling\n        n_embed = embedding.shape[1]\n        n_select = self.embedding_sampling\n        idx = np.random.permutation(n_embed)[:n_select]\n        embedding = embedding[:, idx]\n\n        word_features = np.concatenate(\n            [embedding, word_extra_features], axis=1)\n        return word_features\n\n\n# =======  Training modules  =======\n\nclass Embedding(EmbeddingPresets):\n    pass\n\n\nclass Encoder(EncoderPresets):\n    pass\n\n\nclass Aggregator(AggregatorPresets):\n    pass\n\n\nclass MLP(MLPPresets):\n    pass\n\n\nclass Ensembler(EnsemblerPresets):\n    pass\n\nclass ExperimentConfigBuilder(ExperimentConfigBuilderBase):\n\n    default_config = dict(\n        test=False,\n        device=0,\n        maxlen=72,\n        vocab_mincount=5,\n        scale_batchsize=[],\n        validate_from=4,\n    )\n\n    @property\n    def modules(self):\n        return [\n            TextNormalizer,\n            TextTokenizer,\n            WordExtraFeaturizer,\n            SentenceExtraFeaturizer,\n#             Embedding,\n#             Encoder,\n#             Aggregator,\n#             MLP,\n        ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#exec(modules)\nconfig = ExperimentConfigBuilder().build(args=[])\nprint(config)\nstart = time.time()\nset_seed(config.seed)\n\ntrain_df, submit_df = load_qiqc(n_rows=config.n_rows)\ndatasets = build_datasets(train_df, submit_df, config.holdout, config.seed)\ntrain_dataset, test_dataset, submit_dataset = datasets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint('Tokenize texts...')\npreprocessor = Preprocessor()\nnormalizer = TextNormalizer(config)\ntokenizer = TextTokenizer(config)\ntrain_dataset.tokens, test_dataset.tokens, submit_dataset.tokens = \\\n    preprocessor.tokenize(datasets, normalizer, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint('Build vocabulary...')\nvocab = preprocessor.build_vocab(datasets, config)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint('Build token ids...')\ntrain_dataset.tids, test_dataset.tids, submit_dataset.tids = \\\n    preprocessor.build_tokenids(datasets, vocab, config)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint('Build sentence extra features...')\nsentence_extra_featurizer = SentenceExtraFeaturizer(config)\ntrain_dataset._X2, test_dataset._X2, submit_dataset._X2 = \\\n    preprocessor.build_sentence_features(\n        datasets, sentence_extra_featurizer)\n[d.build(config.device) for d in datasets]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%time\n#print('Load pretrained vectors...')\n#pretrained_vectors = load_pretrained_vectors(\n #   config.use_pretrained_vectors, vocab.token2id, test=config.test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# print('Build word embedding matrix...')\n# word_embedding_featurizer = WordEmbeddingFeaturizer(config, vocab)\n# embedding_matrices = preprocessor.build_embedding_matrices(\n#     datasets, word_embedding_featurizer, vocab, pretrained_vectors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint('Build word extra features...')\nword_extra_featurizer = WordExtraFeaturizer(config, vocab)\nword_extra_features = word_extra_featurizer(vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(word_extra_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(vocab.word_freq.keys()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\n# del config,start,train_df,submit_df,datasets,train_dataset,test_dataset,submit_dataset,preprocessor,normalizer,tokenizer; gc.collect()\ngc.collect() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Garbage cleaned\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# config,start,train_df,submit_df,datasets,train_dataset,test_dataset,submit_dataset,preprocessor,normalizer,tokenizer\n\n# %%time\n# #exec(modules)\n# config = ExperimentConfigBuilder().build(args=[])\n# print(config)\n# start = time.time()\n# set_seed(config.seed)\n\n# train_df, submit_df = load_qiqc(n_rows=config.n_rows)\n# datasets = build_datasets(train_df, submit_df, config.holdout, config.seed)\n# train_dataset, test_dataset, submit_dataset = datasets\n\n# %%time\n# print('Tokenize texts...')\n# preprocessor = Preprocessor()\n# normalizer = TextNormalizer(config)\n# tokenizer = TextTokenizer(config)\n# train_dataset.tokens, test_dataset.tokens, submit_dataset.tokens = \\\n#     preprocessor.tokenize(datasets, normalizer, tokenizer)\n\n# %%time\n# print('Build vocabulary...')\n# vocab = preprocessor.build_vocab(datasets, config)\n\n# %%time\n# print('Build token ids...')\n# train_dataset.tids, test_dataset.tids, submit_dataset.tids = \\\n#     preprocessor.build_tokenids(datasets, vocab, config)\n\n# %%time\n# print('Build sentence extra features...')\n# sentence_extra_featurizer = SentenceExtraFeaturizer(config)\n# train_dataset._X2, test_dataset._X2, submit_dataset._X2 = \\\n#     preprocessor.build_sentence_features(\n#         datasets, sentence_extra_featurizer)\n# [d.build(config.device) for d in datasets]\n    \n# %%time\n# print('Build word extra features...')\n# word_extra_featurizer = WordExtraFeaturizer(config, vocab)\n# word_extra_features = word_extra_featurizer(vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(vars(vocab))\n# import json\n# print json.dumps(vars(vocab),sort_keys=True, indent=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%time\n# print('Build models...')\n# word_features_cv = [\n#     preprocessor.build_word_features(\n#         word_embedding_featurizer, embedding_matrices, word_extra_features)\n#     for i in range(config.cv)]\n\n# models = [\n#     build_model(\n#         config, word_features, sentence_extra_featurizer.n_dims\n#     ) for word_features in word_features_cv]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(models[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# print('Start training...')\n# splitter = sklearn.model_selection.StratifiedKFold(\n#     n_splits=config.cv, shuffle=True, random_state=config.seed)\n# train_results, valid_results = [], []\n# best_models = []\n\n# for i_cv, (train_indices, valid_indices) in enumerate(\n#         splitter.split(train_dataset.df, train_dataset.df.target)):\n#     if config.cv_part is not None and i_cv >= config.cv_part:\n#         break\n#     train_tensor = train_dataset.build_labeled_dataset(train_indices)\n#     valid_tensor = train_dataset.build_labeled_dataset(valid_indices)\n#     valid_iter = DataLoader(\n#         valid_tensor, batch_size=config.batchsize_valid)\n\n#     model = models.pop(0)\n#     model = model.to_device(config.device)\n#     model_snapshots = []\n#     optimizer = torch.optim.Adam(model.parameters(), config.lr)\n#     train_result = ClassificationResult('train', config.outdir, str(i_cv))\n#     valid_result = ClassificationResult('valid', config.outdir, str(i_cv))\n\n#     batchsize = config.batchsize\n#     for epoch in range(config.epochs):\n#         if epoch in config.scale_batchsize:\n#             batchsize *= 2\n#             print(f'Batchsize: {batchsize}')\n#         epoch_start = time.time()\n#         sampler = None\n#         train_iter = DataLoader(\n#             train_tensor, sampler=sampler, drop_last=True,\n#             batch_size=batchsize, shuffle=sampler is None)\n#         _summary = []\n\n#         # Training loop\n#         for i, batch in enumerate(\n#                 tqdm(train_iter, desc='train', leave=False)):\n#             model.train()\n#             optimizer.zero_grad()\n#             loss, output = model.calc_loss(*batch)\n#             loss.backward()\n#             optimizer.step()\n#             train_result.add_record(**output)\n#         train_result.calc_score(epoch)\n#         _summary.append(train_result.summary.iloc[-1])\n\n#         # Validation loop\n#         if epoch >= config.validate_from:\n#             for i, batch in enumerate(\n#                     tqdm(valid_iter, desc='valid', leave=False)):\n#                 model.eval()\n#                 loss, output = model.calc_loss(*batch)\n#                 valid_result.add_record(**output)\n#             valid_result.calc_score(epoch)\n#             _summary.append(valid_result.summary.iloc[-1])\n\n#             _model = deepcopy(model)\n#             _model.threshold = valid_result.summary.threshold[epoch]\n#             model_snapshots.append(_model)\n\n#         summary = pd.DataFrame(_summary).set_index('name')\n#         epoch_time = time.time() - epoch_start\n#         pbar = '#' * (i_cv + 1) + '-' * (config.cv - 1 - i_cv)\n#         tqdm.write(f'\\n{pbar} cv: {i_cv} / {config.cv}, epoch {epoch}, '\n#                    f'time: {epoch_time}')\n#         tqdm.write(str(summary))\n\n#     train_results.append(train_result)\n#     valid_results.append(valid_result)\n#     best_indices = valid_result.summary.fbeta.argsort()[::-1]\n#     best_models.extend([model_snapshots[i] for i in\n#                         best_indices[:config.ensembler_n_snapshots]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Build ensembler\n# train_X, train_X2, train_t = \\\n#     train_dataset.X, train_dataset.X2, train_dataset.t\n# ensembler = Ensembler(config, best_models, valid_results)\n# ensembler.fit(train_X, train_X2, train_t)\n# scores = dict(\n#     valid_fbeta=np.array([r.best_fbeta for r in valid_results]).mean(),\n#     valid_epoch=np.array([r.best_epoch for r in valid_results]).mean(),\n#     threshold_cv=ensembler.threshold_cv,\n#     threshold=ensembler.threshold,\n#     elapsed_time=time.time() - start,\n# )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if config.holdout:\n#     test_X, test_X2, test_t = \\\n#         test_dataset.X, test_dataset.X2, test_dataset._t\n#     y, t = ensembler.predict_proba(test_X, test_X2), test_t\n#     y_pred = y > ensembler.threshold\n#     y_pred_cv = y > ensembler.threshold_cv\n#     result = classification_metrics(y_pred, t)\n#     result_cv = classification_metrics(y_pred_cv, t)\n#     result_theoretical = classification_metrics(y, t)\n#     scores.update(dict(\n#         test_fbeta=result['fbeta'],\n#         test_fbeta_cv=result_cv['fbeta'],\n#         test_fbeta_theoretical=result_theoretical['fbeta'],\n#         test_threshold_theoretical=result_theoretical['threshold'],\n#     ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Predict submit datasets\n# submit_y = ensembler.predict(submit_dataset.X, submit_dataset.X2)\n# submit_df['prediction'] = submit_y\n# submit_df = submit_df[['qid', 'prediction']]\n# submit_df.to_csv(config.outdir / 'submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i, (train, valid) in enumerate(zip(train_results, valid_results)):\n#     df = pd.concat([train.summary, valid.summary])\n#     df = df.set_index(['name', df.index], drop=True)\n#     display(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pd.DataFrame([scores])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n## Settings:\n# some config values \nmax_features = 90000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 50 # max number of words in a question to use","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport time\nimport tensorflow as tf\nimport numpy as np # linear algebra\nimport random\nimport os \nos.environ['PYTHONHASHSEED'] = '11'\nnp.random.seed(22)\nrandom.seed(33)\ntf.set_random_seed(44)\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport gc\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, Embedding, CuDNNGRU\nfrom keras.layers import Bidirectional, GlobalMaxPooling1D\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.initializers import glorot_uniform\nfrom keras.callbacks import Callback\nfrom keras.models import clone_model\nimport keras.backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t0 = time.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nprint(\"Train shape : \",train_df.shape)\nprint(\"Test shape : \",test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install -U nltk\nimport nltk\nnltk.download()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords=nltk.corpus.stopwords.words('english')\n## split to train and val\ntrain_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)\n\n## fill up the missing values\ntrain_X = train_df[\"question_text\"].fillna(\"_na_\").values\nval_X = val_df[\"question_text\"].fillna(\"_na_\").values\ntest_X = test_df[\"question_text\"].fillna(\"_na_\").values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\ntrain_X = train_df['question_text']\n# train_X = train_df[\"question_text\"].fillna(\"_na_\").values\ntrain_X = train_X.tolist()\nval_X = val_df['question_text']\n#val_X = val_df[\"question_text\"].fillna(\"_na_\").values\nval_X = val_X.tolist()\n\n# test_X = test_df[\"question_text\"].fillna(\"_na_\").values\n\nqid_train = train_df['qid']\nqid_train = qid_train.tolist()\nqid_val = val_df['qid']\nqid_val = qid_val.tolist()\n\nprint(len(train_X))\n\ntest_X = test_df['question_text']\ntest_X = test_X.tolist()\n\nstemming_result_train = {}\nstemming_result_val = {}\nps = PorterStemmer()\nfor i in range(0,len(qid_train)):\n    words = word_tokenize(train_X[i])\n    stemming_result_train[qid_train[i]] = []\n    for w in words:\n        if w not in stopwords:\n            stemming_result_train[qid_train[i]].append(ps.stem(w))\n\nfor i in range(len(qid_val)):\n    words = word_tokenize(val_X[i])\n    stemming_result_val[qid_val[i]] = []\n    for w in words:\n        if w not in stopwords:\n            stemming_result_val[qid_val[i]].append(ps.stem(w))\n\nstemming_result_test = []\nfor i in range(len(test_X)):\n    for w in test_X[i]:\n        stemming_result_test.append(ps.stem(w))\n    \n# stemming_result_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nstemmed_content_train = {}\nlowered_content_train = {}\nalphabet_content_train = {}\nfor qid, words in stemming_result_train.items():\n    stemmed_content_train[qid] = \" \".join(str(word) for word in words)\n    lowered_content_train[qid] = stemmed_content_train[qid].lower()\n    #alphabet_content[qid] = lowered_content[qid].replace('[^a-z ]','')\n    alphabet_content_train[qid] = re.sub(r'[^a-zA-Z ]',\"\", lowered_content_train[qid])\n\nstemmed_content_val = {}\nlowered_content_val = {}\nalphabet_content_val = {}\nfor qid, words in stemming_result_val.items():\n    stemmed_content_val[qid] = \" \".join(str(word) for word in words)\n    lowered_content_val[qid] = stemmed_content_val[qid].lower()\n    #alphabet_content[qid] = lowered_content[qid].replace('[^a-z ]','')\n    alphabet_content_val[qid] = re.sub(r'[^a-zA-Z ]',\"\", lowered_content_val[qid])    \n    \n    \nstemmed_content_test = []\nlowered_content_test = []\nalphabet_content_test = []\nfor i in range(len(stemming_result_test)):\n    stemmed_content_test.append(\" \".join(str(word) for word in stemming_result_test[i]))\n    lowered_content_test.append(stemmed_content_test[i].lower())\n    #alphabet_content[qid] = lowered_content[qid].replace('[^a-z ]','')\n    alphabet_content_test.append(re.sub(r'[^a-zA-Z ]',\"\", lowered_content_test[i]))\npreprocessed_content1_train = []\npreprocessed_content1_val = []\npreprocessed_content1_test = []\nfor qid, text in alphabet_content_train.items():\n    preprocessed_content1_train.append(alphabet_content_train[qid])\nfor qid, text in alphabet_content_val.items():\n    preprocessed_content1_val.append(alphabet_content_val[qid])\n# for qid, text in alphabet_content_test.items():\n#     preprocessed_content1_test.append(alphabet_content_test[qid])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## split to train and val\n# train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)\n\n# ## fill up the missing values\n# train_X = train_df[\"question_text\"].fillna(\"_na_\").values\n# val_X = val_df[\"question_text\"].fillna(\"_na_\").values\n# test_X = test_df[\"question_text\"].fillna(\"_na_\").values\n\n## Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features,\n                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'’“”')\n# tokenizer.fit_on_texts(list(train_X))\n# train_X = tokenizer.texts_to_sequences(train_X)\n# val_X = tokenizer.texts_to_sequences(val_X)\n# test_X = tokenizer.texts_to_sequences(test_X)\n\ntokenizer.fit_on_texts(preprocessed_content1_train)\ntrain_X = tokenizer.texts_to_sequences(preprocessed_content1_train)\nval_X = tokenizer.texts_to_sequences(preprocessed_content1_val)\ntest_X = tokenizer.texts_to_sequences(alphabet_content_test)\n\n## Pad the sentences \ntrunc = 'pre'\ntrain_X = pad_sequences(train_X, maxlen=maxlen, truncating=trunc)\nval_X = pad_sequences(val_X, maxlen=maxlen, truncating=trunc)\ntest_X = pad_sequences(test_X, maxlen=maxlen, truncating=trunc)\n\n## Get the target values\ntrain_y = train_df['target'].values\nval_y = val_df['target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from nltk.stem import PorterStemmer\n# from nltk.tokenize import sent_tokenize, word_tokenize\n\n# question_text_train = train_df['question_text']\n# question_text_train = question_text_train.tolist()\n# question_text_val = val_df['question_text']\n# question_text_val = question_text_val.tolist()\n\n# qid_train = train_df['qid']\n# qid_train = qid_train.tolist()\n# qid_val = val_df['qid']\n# qid_val = qid_val.tolist()\n\n# print(len(question_text_train))\n\n# stemming_result_train = {}\n# stemming_result_val = {}\n# ps = PorterStemmer()\n# for i in range(0,len(qid_train)):\n#     words = word_tokenize(question_text_train[i])\n#     stemming_result_train[qid_train[i]] = []\n#     for w in words:\n#         if w not in stopwords:\n#             stemming_result_train[qid_train[i]].append(ps.stem(w))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix_1 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix_1[i] = embedding_vector\n\ndel embeddings_index; gc.collect() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix_2 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix_2[i] = embedding_vector\n        \ndel embeddings_index; gc.collect() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix_3 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix_3[i] = embedding_vector\n\ndel embeddings_index; gc.collect() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go\nfrom gensim.models import KeyedVectors\n\nEMBEDDING_FILE = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\nembeddings_index = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix_4 = (np.random.rand(nb_words, embed_size) - 0.5) / 5.0\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    if word in embeddings_index:\n        embedding_vector = embeddings_index.get_vector(word)\n        embedding_matrix_4[i] = embedding_vector\n        \ndel embeddings_index; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(vocab.word_freq.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(list(vocab.word_freq.keys()).index(\"youtube\"))\n# print(word_extra_features[485])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# word_extra_features\n# print(len(vocab.word_freq.keys()))\nembed_size = 1\nemb_mean, emb_std = word_extra_features.mean(), word_extra_features.std()\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix_5 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    if word in vocab.word_freq.keys():\n        embedding_vector = word_extra_features[list(vocab.word_freq.keys()).index(word)]\n        embedding_matrix_5[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(embedding_matrix_4.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.concatenate((embedding_matrix_1, embedding_matrix_2, embedding_matrix_3, embedding_matrix_4, embedding_matrix_5), axis=1)\n\ndel embedding_matrix_1, embedding_matrix_2, embedding_matrix_3, embedding_matrix_4, embedding_matrix_5\ngc.collect()\nnp.shape(embedding_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Done preprocessing {time.time() - t0:.1f}s')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ExponentialMovingAverage(Callback):\n    def __init__(self, model, decay=0.999, mode='epoch', n=100):\n        \"\"\"\n        mode: 'epoch': Do update_weights every epoch.\n              'batch':                   every n batches.\n        n   :\n        \"\"\"\n        self.decay = decay\n        self.mode = mode\n        self.ema_model = clone_model(model)\n        self.ema_model.set_weights(model.get_weights())\n        self.n = n\n        if self.mode is 'batch':\n            self.cnt = 0\n        self.ema_weights = [K.get_value(w) for w in model.trainable_weights]\n        self.n_weights = len(self.ema_weights)\n        super(ExponentialMovingAverage, self).__init__()\n\n    def on_batch_end(self, batch, logs={}):\n        if self.mode is 'batch':\n            self.cnt += 1\n            if self.cnt % self.n == 0:\n                self.update_weights()\n\n    def on_epoch_end(self, epoch, logs={}):\n        if self.mode is 'epoch':\n            self.update_weights()\n        for var, w in zip(self.ema_model.trainable_weights, self.ema_weights):\n            K.set_value(var, w)\n\n    def update_weights(self):\n        for w_old, var_new in zip(self.ema_weights, self.model.trainable_weights):\n            w_old += (1 - self.decay) * (K.get_value(var_new) - w_old)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_rnn_model(rnn, maxlen, embedding, max_features, embed_size,\n                     rnn_dim=64, dense1_dim=100, dense2_dim=50,\n                     embed_trainable=False, seed=123):\n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, weights=[embedding],\n                  trainable=embed_trainable)(inp)\n    x = Dense(dense1_dim, activation='relu',\n              kernel_initializer=glorot_uniform(seed=seed))(x)\n    x = Bidirectional(rnn(rnn_dim, return_sequences=True,\n                          kernel_initializer=glorot_uniform(seed=seed)))(x)\n    x = GlobalMaxPooling1D()(x)\n    x = Dense(dense2_dim, activation='relu',\n              kernel_initializer=glorot_uniform(seed=seed))(x)\n    x = Dense(1, activation='sigmoid',\n              kernel_initializer=glorot_uniform(seed=seed))(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def f1_best(y_val, pred_val):\n    best_f1 = 0\n    best_thresh = 0\n    for thresh in np.linspace(0.2, 0.4, 41):\n        f1 = metrics.f1_score(y_val, (pred_val > thresh).astype(int))\n        if f1 > best_f1:\n            best_f1 = f1\n            best_thresh = thresh\n    return best_f1, best_thresh\n\nembed_ids = [list(range(300)), list(range(300, 600)),\n             list(range(600, 900)), list(range(900, 1200))]\nembed_ids_dict = {1: [embed_ids[0], embed_ids[1], embed_ids[2], embed_ids[3]],\n                  2: [embed_ids[0] + embed_ids[1],\n                      embed_ids[0] + embed_ids[2],\n                      embed_ids[0] + embed_ids[3],\n                      embed_ids[1] + embed_ids[2],\n                      embed_ids[1] + embed_ids[3],\n                      embed_ids[2] + embed_ids[3]],\n                  3: [embed_ids[0] + embed_ids[1] + embed_ids[2],\n                      embed_ids[0] + embed_ids[1] + embed_ids[3],\n                      embed_ids[0] + embed_ids[2] + embed_ids[3],\n                      embed_ids[1] + embed_ids[2] + embed_ids[3]],\n                  4: [embed_ids[0] + embed_ids[1] + embed_ids[2] + embed_ids[3]]}\nembed_ids_lst = embed_ids_dict[2]\nembed_size = 600\n\nrnn = CuDNNGRU\nembed_trainable = False\n\nn_models = 6\nepochs = 7\nbatch_size = 512\ndense1_dim = rnn_dim = 128\ndense2_dim = 2 * rnn_dim\n\nema_n = int(len(train_y) / batch_size / 10)\ndecay = 0.9\nscores = []\n\npred_avg = np.zeros((len(val_y), 1))\npred_test_avg = np.zeros((test_df.shape[0], 1))\nfor i in range(n_models):\n    t1 = time.time()\n    seed = 101 + 11 * i\n    cols_in_use = embed_ids_lst[i % len(embed_ids_lst)]\n    model = create_rnn_model(rnn, maxlen, embedding_matrix[:, cols_in_use],\n                             max_features, embed_size,\n                             rnn_dim=rnn_dim,\n                             dense1_dim=dense1_dim,\n                             dense2_dim=dense2_dim,\n                             embed_trainable=embed_trainable,\n                             seed=seed)\n    ema = ExponentialMovingAverage(model, decay=decay, mode='batch', n=ema_n)\n    model.fit(train_X, train_y, batch_size=batch_size, epochs=epochs,\n              callbacks=[ema], verbose=0)\n    m = ema.ema_model\n    t_per_epoch = (time.time() - t1) / epochs\n    pred = m.predict([val_X])\n    pred_avg += pred\n    pred_test = m.predict([test_X])\n    pred_test_avg += pred_test\n    f1_one, thresh_one = f1_best(val_y, pred)\n    f1_avg, thresh_avg = f1_best(val_y, pred_avg / (i + 1))\n    nll_one = metrics.log_loss(val_y, pred)\n    nll_avg = metrics.log_loss(val_y, pred_avg / (i + 1))\n    auc_one = metrics.roc_auc_score(val_y, pred)\n    auc_avg = metrics.roc_auc_score(val_y, pred_avg)\n    print(f'  n_model:{i + 1} epoch:{epochs} F1:{f1_avg:.4f} th:{thresh_avg:.3f} ' +\n          f'AUC:{auc_avg:.4f} NLL:{nll_avg:.4f} One:{f1_one:.4f} {auc_one:.4f} {nll_one:.4f} ' +\n          f'Time:{time.time() - t1:.1f}s  {t_per_epoch:.1f}s/epoch')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test_avg /= n_models\npred_test_avg = (pred_test_avg>thresh_avg).astype(int)\nout_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\nout_df['prediction'] = pred_test_avg\nout_df.to_csv(\"submission.csv\", index=False)\n\nprint(f'Done:{time.time() - t0:.1f}s')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(out_df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}