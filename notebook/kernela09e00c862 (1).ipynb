{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import argparse\nimport json\nimport math\nimport os\nimport sys\nimport unidecode\nimport random\nimport re\nimport time\nimport yaml\nfrom abc import ABCMeta, abstractmethod\nfrom collections import defaultdict, Counter\nfrom copy import deepcopy\nfrom functools import partial\nfrom multiprocessing import Pool\nfrom pathlib import Path\n\nimport nltk\nimport gensim\nimport sklearn\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nfrom gensim.corpora import Dictionary\nfrom gensim.models import KeyedVectors\nfrom gensim.models import Word2Vec, Doc2Vec, FastText\nfrom sklearn import metrics\nfrom torch import nn\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\nfrom tqdm import tqdm\n%load_ext Cython","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class QIQCDataset(object):\n\n    def __init__(self, df):\n        self.df = df\n\n    @property\n    def tokens(self):\n        return self.df.tokens.values\n\n    @tokens.setter\n    def tokens(self, tokens):\n        self.df['tokens'] = tokens\n\n    @property\n    def positives(self):\n        return self.df[self.df.target == 1]\n\n    @property\n    def negatives(self):\n        return self.df[self.df.target == 0]\n\n    def build(self, device):\n        self._X = self.tids\n        self.X = torch.Tensor(self._X).type(torch.long).to(device)\n        if 'target' in self.df:\n            self._t = self.df.target[:, None]\n            self._W = self.df.weights\n            self.t = torch.Tensor(self._t).type(torch.float).to(device)\n            self.W = torch.Tensor(self._W).type(torch.float).to(device)\n        if hasattr(self, '_X2'):\n            self.X2 = torch.Tensor(self._X2).type(torch.float).to(device)\n        else:\n            self._X2 = np.zeros((self._X.shape[0], 1), 'f')\n            self.X2 = torch.Tensor(self._X2).type(torch.float).to(device)\n\n    def build_labeled_dataset(self, indices):\n        return torch.utils.data.TensorDataset(\n            self.X[indices], self.X2[indices],\n            self.t[indices], self.W[indices])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_qiqc(n_rows=None):\n    train_df = pd.read_csv(\"../input/train.csv\", nrows=n_rows)\n    submit_df = pd.read_csv(\"../input/test.csv\", nrows=n_rows)\n    n_labels = {\n        0: (train_df.target == 0).sum(),\n        1: (train_df.target == 1).sum(),\n    }\n    train_df['target'] = train_df.target.astype('f')\n    train_df['weights'] = train_df.target.apply(lambda t: 1 / n_labels[t])\n\n    return train_df, submit_df\n\n\ndef build_datasets(train_df, submit_df, holdout=False, seed=0):\n    submit_dataset = QIQCDataset(submit_df)\n    if holdout:\n        # Train : Test split for holdout training\n        splitter = sklearn.model_selection.StratifiedShuffleSplit(\n            n_splits=1, test_size=0.1, random_state=seed)\n        train_indices, test_indices = list(splitter.split(\n            train_df, train_df.target))[0]\n        train_indices.sort(), test_indices.sort()\n        train_dataset = QIQCDataset(\n            train_df.iloc[train_indices].reset_index(drop=True))\n        test_dataset = QIQCDataset(\n            train_df.iloc[test_indices].reset_index(drop=True))\n    else:\n        train_dataset = QIQCDataset(train_df)\n        test_dataset = QIQCDataset(train_df.head(0))\n\n    return train_dataset, test_dataset, submit_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%cython\nimport re\n\nNORMALIZER_REGISTRY = {}\n#TOKENIZER_REGISTRY = {}\n#WORD_EMBEDDING_FEATURIZER_REGISTRY = {}\nWORD_EXTRA_FEATURIZER_REGISTRY = {}\nSENTENCE_EXTRA_FEATURIZER_REGISTRY = {}\n\n# Registries for training\nENCODER_REGISTRY = {}\nAGGREGATOR_REGISTRY = {}\nATTENTION_REGISTRY = {}\n\n\ndef register_preprocessor(name):\n    def register_cls(cls):\n        NORMALIZER_REGISTRY[name] = cls\n        return cls\n    return register_cls\n\n\n'''\ndef register_tokenizer(name):\n    def register_cls(cls):\n        TOKENIZER_REGISTRY[name] = cls\n        return cls\n    return register_cls\n'''    \n\ndef register_word_extra_features(name):\n    def register_cls(cls):\n        WORD_EXTRA_FEATURIZER_REGISTRY[name] = cls\n        return cls\n    return register_cls\n\ndef register_sentence_extra_features(name):\n    def register_cls(cls):\n        SENTENCE_EXTRA_FEATURIZER_REGISTRY[name] = cls\n        return cls\n    return register_cls\n\ndef register_encoder(name):\n    def register_cls(cls):\n        ENCODER_REGISTRY[name] = cls\n        return cls\n    return register_cls\n\n\ndef register_aggregator(name):\n    def register_cls(cls):\n        AGGREGATOR_REGISTRY[name] = cls\n        return cls\n    return register_cls\n\n\ndef register_attention(name):\n    def register_cls(cls):\n        ATTENTION_REGISTRY[name] = cls\n        return cls\n    return register_cls\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%cython\nimport re\n\n\n\ncpdef str cylower(str x):\n    return x.lower()\n\ncdef class StringReplacer:\n    cpdef public dict rule\n    cpdef list keys\n    cpdef list values\n    cpdef int n_rules\n\n    def __init__(self, dict rule):\n        self.rule = rule\n        self.keys = list(rule.keys())\n        self.values = list(rule.values())\n        self.n_rules = len(rule)\n\n    def __call__(self, str x):\n        cdef int i\n        for i in range(self.n_rules):\n            if self.keys[i] in x:\n                x = x.replace(self.keys[i], self.values[i])\n        return x\n\n    def __getstate__(self):\n        return (self.rule, self.keys, self.values, self.n_rules)\n\n    def __setstate__(self, state):\n        self.rule, self.keys, self.values, self.n_rules = state\n        \n        \nclass PunctSpacer(StringReplacer):\n\n    def __init__(self, edge_only=False):\n        puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', '█', '½', '…', '“', '★', '”', '–', '●', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', '¯', '♦', '¤', '▲', '¸', '¾', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]  # NOQA\n        if edge_only:\n            rule = {\n                **dict([(f' {p}', f' {p} ') for p in puncts]),\n                **dict([(f'{p} ', f' {p} ') for p in puncts]),\n            }\n        else:\n            rule = dict([(p, f' {p} ') for p in puncts])\n        super().__init__(rule)\n        \n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%cython\nimport re\nCache = {}\nis_alphabet = re.compile(r'[a-zA-Z]')\n\ncpdef str unidecode_weak(str string):\n    \"\"\"Transliterate an Unicode object into an ASCII string\n    >>> unidecode(u\"\\u5317\\u4EB0\")\n    \"Bei Jing \"\n    \"\"\"\n\n    cdef list retval = []\n    cdef int i = 0\n    cdef int n = len(string)\n    cdef str char\n\n    for i in range(n):\n        char = string[i]\n        codepoint = ord(char)\n\n        if codepoint < 0x80: # Basic ASCII\n            retval.append(char)\n            continue\n\n        if codepoint > 0xeffff:\n            continue  # Characters in Private Use Area and above are ignored\n\n        section = codepoint >> 8   # Chop off the last two hex digits\n        position = codepoint % 256 # Last two hex digits\n\n        try:\n            table = Cache[section]\n        except KeyError:\n            try:\n                mod = __import__('unidecode.x%03x'%(section), [], [], ['data'])\n            except ImportError:\n                Cache[section] = None\n                continue   # No match: ignore this character and carry on.\n\n            Cache[section] = table = mod.data\n\n        if table and len(table) > position:\n            if table[position] == '[?]' or is_alphabet.match(table[position]):\n                retval.append(' ' + char + ' ')\n            else:\n                retval.append(table[position])\n\n    return ''.join(retval)    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%cython\nimport re\ncdef class RegExpReplacer:\n    cdef dict rule\n    cdef list keys\n    cdef list values\n    cdef regexp\n    cdef int n_rules\n\n    def __init__(self, dict rule):\n        self.rule = rule\n        self.keys = list(rule.keys())\n        self.values = list(rule.values())\n        self.regexp = re.compile('(%s)' % '|'.join(self.keys))\n        self.n_rules = len(rule)\n\n    @property\n    def rule(self):\n        return self.rule\n\n    def __call__(self, str x):\n        def replace(match):\n            x = match.group(0)\n            if x in self.rule:\n                return self.rule[x]\n            else:\n                for i in range(self.n_rules):\n                    x = re.sub(self.keys[i], self.values[i], x)\n                return x\n        return self.regexp.sub(replace, x)     \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nclass NumberReplacer(RegExpReplacer):\n\n    def __init__(self, with_underscore=False):\n        prefix, suffix = '', ''\n        if with_underscore:\n            prefix += ' __'\n            suffix = '__ '\n        rule = {\n            '[0-9]{5,}': f'{prefix}#####{suffix}',\n            '[0-9]{4}': f'{prefix}####{suffix}',\n            '[0-9]{3}': f'{prefix}###{suffix}',\n            '[0-9]{2}': f'{prefix}##{suffix}',\n        }\n        super().__init__(rule)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MisspellReplacer(StringReplacer):\n\n    def __init__(self):\n        rule = {\n            \"ain't\": \"is not\",\n            \"aren't\": \"are not\",\n            \"can't\": \"cannot\",\n            \"'cause\": \"because\",\n            \"could've\": \"could have\",\n            \"couldn't\": \"could not\",\n            \"didn't\": \"did not\",\n            \"doesn't\": \"does not\",\n            \"don't\": \"do not\",\n            \"hadn't\": \"had not\",\n            \"hasn't\": \"has not\",\n            \"haven't\": \"have not\",\n            \"he'd\": \"he would\",\n            \"he'll\": \"he will\",\n            \"he's\": \"he is\",\n            \"how'd'y\": \"how do you\",\n            \"how'd\": \"how did\",\n            \"how'll\": \"how will\",\n            \"how's\": \"how is\",\n            \"i'd've\": \"i would have\",\n            \"i'd\": \"i would\",\n            \"i'll've\": \"i will have\",\n            \"i'll\": \"i will\",\n            \"i'm\": \"i am\",\n            \"i've\": \"i have\",\n            \"isn't\": \"is not\",\n            \"it'd've\": \"it would have\",\n            \"it'd\": \"it would\",\n            \"it'll've\": \"it will have\",\n            \"it'll\": \"it will\",\n            \"it's\": \"it is\",\n            \"let's\": \"let us\",\n            \"ma'am\": \"madam\",\n            \"mayn't\": \"may not\",\n            \"might've\": \"might have\",\n            \"mightn't've\": \"might not have\",\n            \"mightn't\": \"might not\",\n            \"must've\": \"must have\",\n            \"mustn't've\": \"must not have\",\n            \"mustn't\": \"must not\",\n            \"needn't've\": \"need not have\",\n            \"needn't\": \"need not\",\n            \"o'clock\": \"of the clock\",\n            \"oughtn't've\": \"ought not have\",\n            \"oughtn't\": \"ought not\",\n            \"shan't've\": \"shall not have\",\n            \"shan't\": \"shall not\",\n            \"sha'n't\": \"shall not\",\n            \"she'd've\": \"she would have\",\n            \"she'd\": \"she would\",\n            \"she'll've\": \"she will have\",\n            \"she'll\": \"she will\",\n            \"she's\": \"she is\",\n            \"should've\": \"should have\",\n            \"shouldn't've\": \"should not have\",\n            \"shouldn't\": \"should not\",\n            \"so've\": \"so have\",\n            \"so's\": \"so as\",\n            \"this's\": \"this is\",\n            \"that'd've\": \"that would have\",\n            \"that'd\": \"that would\",\n            \"that's\": \"that is\",\n            \"there'd've\": \"there would have\",\n            \"there'd\": \"there would\",\n            \"there's\": \"there is\",\n            \"here's\": \"here is\",\n            \"they'd've\": \"they would have\",\n            \"they'd\": \"they would\",\n            \"they'll've\": \"they will have\",\n            \"they'll\": \"they will\",\n            \"they're\": \"they are\",\n            \"they've\": \"they have\",\n            \"to've\": \"to have\",\n            \"wasn't\": \"was not\",\n            \"we'd've\": \"we would have\",\n            \"we'd\": \"we would\",\n            \"we'll've\": \"we will have\",\n            \"we'll\": \"we will\",\n            \"we're\": \"we are\",\n            \"we've\": \"we have\",\n            \"weren't\": \"were not\",\n            \"what'll've\": \"what will have\",\n            \"what'll\": \"what will\",\n            \"what're\": \"what are\",\n            \"what's\": \"what is\",\n            \"what've\": \"what have\",\n            \"when's\": \"when is\",\n            \"when've\": \"when have\",\n            \"where'd\": \"where did\",\n            \"where's\": \"where is\",\n            \"where've\": \"where have\",\n            \"who'll've\": \"who will have\",\n            \"who'll\": \"who will\",\n            \"who's\": \"who is\",\n            \"who've\": \"who have\",\n            \"why's\": \"why is\",\n            \"why've\": \"why have\",\n            \"will've\": \"will have\",\n            \"won't've\": \"will not have\",\n            \"won't\": \"will not\",\n            \"would've\": \"would have\",\n            \"wouldn't've\": \"would not have\",\n            \"wouldn't\": \"would not\",\n            \"y'all'd've\": \"you all would have\",\n            \"y'all'd\": \"you all would\",\n            \"y'all're\": \"you all are\",\n            \"y'all've\": \"you all have\",\n            \"y'all\": \"you all\",\n            \"you'd've\": \"you would have\",\n            \"you'd\": \"you would\",\n            \"you'll've\": \"you will have\",\n            \"you'll\": \"you will\",\n            \"you're\": \"you are\",\n            \"you've\": \"you have\",\n            \"colour\": \"color\",\n            \"centre\": \"center\",\n            \"favourite\": \"favorite\",\n            \"travelling\": \"traveling\",\n            \"counselling\": \"counseling\",\n            \"theatre\": \"theater\",\n            \"cancelled\": \"canceled\",\n            \"labour\": \"labor\",\n            \"organisation\": \"organization\",\n            \"wwii\": \"world war 2\",\n            \"citicise\": \"criticize\",\n            \"youtu \": \"youtube \",\n            \"qoura\": \"quora\",\n            \"sallary\": \"salary\",\n            \"whta\": \"what\",\n            \"narcisist\": \"narcissist\",\n            \"howdo\": \"how do\",\n            \"whatare\": \"what are\",\n            \"howcan\": \"how can\",\n            \"howmuch\": \"how much\",\n            \"howmany\": \"how many\",\n            \"whydo\": \"why do\",\n            \"doi\": \"do i\",\n            \"thebest\": \"the best\",\n            \"howdoes\": \"how does\",\n            \"mastrubation\": \"masturbation\",\n            \"mastrubate\": \"masturbate\",\n            \"mastrubating\": \"masturbating\",\n            \"pennis\": \"penis\",\n            \"etherium\": \"ethereum\",\n            \"narcissit\": \"narcissist\",\n            \"bigdata\": \"big data\",\n            \"2k17\": \"2017\",\n            \"2k18\": \"2018\",\n            \"qouta\": \"quota\",\n            \"exboyfriend\": \"ex boyfriend\",\n            \"airhostess\": \"air hostess\",\n            \"whst\": \"what\",\n            \"watsapp\": \"whatsapp\",\n            \"demonitisation\": \"demonetization\",\n            \"demonitization\": \"demonetization\",\n            \"demonetisation\": \"demonetization\",\n        }\n        super().__init__(rule)   \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class KerasFilterReplacer(StringReplacer):\n\n    def __init__(self):\n        filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n        rule = dict([(f, ' ') for f in filters])\n        super().__init__(rule)        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"register_preprocessor('lower')(cylower)\nregister_preprocessor('punct')(PunctSpacer())\n#register_preprocessor('unidecode')(unidecode)\nregister_preprocessor('unidecode_weak')(unidecode_weak)\nregister_preprocessor('number')(NumberReplacer())\nregister_preprocessor('number+underscore')(\n    NumberReplacer(with_underscore=True))\nregister_preprocessor('misspell')(MisspellReplacer())\nregister_preprocessor('keras')(KerasFilterReplacer())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%cython\ncpdef list cysplit(str x):\n    return x.split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TOKENIZER_REGISTRY = {}\ndef register_tokenizer(name):\n    def register_cls(cls):\n        TOKENIZER_REGISTRY[name] = cls\n        return cls\n    return register_cls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"register_tokenizer('space')(cysplit)\nregister_tokenizer('word_tokenize')(nltk.word_tokenize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextNormalizerWrapper(object):\n\n    registry = NORMALIZER_REGISTRY\n    default_config = None\n\n    def __init__(self, config):\n        self.normalizers = [self.registry[n] for n in config.normalizers]\n\n    @classmethod\n    def add_args(cls, parser):\n        assert isinstance(cls.default_config, dict)\n        parser.add_argument(\n            '--normalizers', nargs='+', choices=cls.registry)\n        parser.set_defaults(**cls.default_config)\n\n    def __call__(self, x):\n        for normalizer in self.normalizers:\n            x = normalizer(x)\n        return x    \n\n\n\n\n\nclass TextNormalizerPresets(TextNormalizerWrapper):\n\n    default_config = dict(\n        normalizers=[\n            'lower',\n            'misspell',\n            'punct',\n            'number+underscore'\n        ]\n    )\n\n\n\n\nclass TextNormalizer(TextNormalizerPresets):\n    pass\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ExperimentConfigBuilderBase(metaclass=ABCMeta):\n\n    default_config = None\n\n    def add_args(self, parser):\n        parser.add_argument('--modelfile', '-m', type=Path)\n        parser.add_argument('--outdir-top', type=Path, default=Path('results'))\n        parser.add_argument('--outdir-bottom', type=str, default='default')\n        parser.add_argument('--device', '-g', type=int)\n        parser.add_argument('--test', action='store_true')\n        parser.add_argument('--logging', action='store_true')\n        parser.add_argument('--n-rows', type=int)\n\n        parser.add_argument('--seed', type=int, default=1029)\n        parser.add_argument('--optuna-trials', type=int)\n        parser.add_argument('--gridsearch', action='store_true')\n        parser.add_argument('--holdout', action='store_true')\n        parser.add_argument('--cv', type=int, default=5)\n        parser.add_argument('--cv-part', type=int)\n        parser.add_argument('--processes', type=int, default=2)\n\n        parser.add_argument('--lr', type=float, default=1e-3)\n        parser.add_argument('--batchsize', type=int, default=512)\n        parser.add_argument('--batchsize-valid', type=int, default=1024)\n        parser.add_argument('--scale-batchsize', type=int, nargs='+',\n                            default=[])\n        parser.add_argument('--epochs', type=int, default=5)\n        parser.add_argument('--validate-from', type=int)\n        parser.add_argument('--pos-weight', type=float, default=1.)\n        parser.add_argument('--maxlen', type=float, default=72)\n        parser.add_argument('--vocab-mincount', type=float, default=5)\n        parser.add_argument('--ensembler-n-snapshots', type=int, default=1)\n\n    @abstractmethod\n    def modules(self):\n        raise NotImplementedError()\n\n    def build(self, args=None):\n        assert self.default_config is not None\n        parser = argparse.ArgumentParser()\n        self.add_args(parser)\n        parser.set_defaults(**self.default_config)\n\n        for module in self.modules:\n            module.add_args(parser)\n        config, extra_config = parser.parse_known_args(args)\n\n        for module in self.modules:\n            if hasattr(module, 'add_extra_args'):\n                module.add_extra_args(parser, config)\n\n        if config.test:\n            parser.set_defaults(**dict(\n                n_rows=500,\n                batchsize=64,\n                validate_from=0,\n                epochs=3,\n                cv_part=2,\n                ensembler_test_size=1.,\n            ))\n\n        config = parser.parse_args(args)\n        if config.modelfile is not None:\n            config.outdir = config.outdir_top / config.modelfile.stem \\\n                / config.outdir_bottom\n        else:\n            config.outdir = Path('.')\n\n        return config","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextTokenizerWrapper(object):\n\n    registry = TOKENIZER_REGISTRY\n    default_config = None\n\n    def __init__(self, config):\n        self.tokenizer = self.registry[config.tokenizer]\n\n    @classmethod\n    def add_args(cls, parser):\n        assert isinstance(cls.default_config, dict)\n        parser.add_argument('--tokenizer', choices=cls.registry)\n        parser.set_defaults(**cls.default_config)\n\n    def __call__(self, x):\n        return self.tokenizer(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextTokenizerPresets(TextTokenizerWrapper):\n\n    default_config = dict(\n        tokenizer='space'\n    )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextTokenizer(TextTokenizerPresets):\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class WordExtraFeaturizerWrapper(object):\n\n    registry = WORD_EXTRA_FEATURIZER_REGISTRY\n    default_config = None\n\n    def __init__(self, config, vocab):\n        self.config = config\n        self.vocab = vocab\n        self.featurizers = {\n            k: self.registry[k]() for k in config.word_extra_features}\n\n    @classmethod\n    def add_args(cls, parser):\n        parser.add_argument(\n            '--word-extra-features', nargs='+', choices=cls.registry)\n        parser.set_defaults(**cls.default_config)\n\n    def __call__(self, vocab):\n        empty = np.empty([len(vocab), 0])\n        return np.concatenate([empty, *[\n            f(vocab) for f in self.featurizers.values()]], axis=1)\n    \n    \nclass WordExtraFeaturizerPresets(WordExtraFeaturizerWrapper):\n\n    default_config = dict(\n        word_extra_features=[],\n    )\n\n    \n    \nclass WordExtraFeaturizer(WordExtraFeaturizerPresets):\n\n    default_config = dict(\n        word_extra_features=['idf', 'unk'],\n    )    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class WordVocab(object):\n\n    def __init__(self, mincount=1):\n        self.counter = Counter()\n        self.n_documents = 0\n        self._counters = {}\n        self._n_documents = defaultdict(int)\n        self.mincount = mincount\n\n    def __len__(self):\n        return len(self.token2id)\n\n    def add_documents(self, documents, name):\n        self._counters[name] = Counter()\n        for document in documents:\n            bow = dict.fromkeys(document, 1)\n            self._counters[name].update(bow)\n            self.counter.update(bow)\n            self.n_documents += 1\n            self._n_documents[name] += 1\n\n    def build(self):\n        counter = dict(self.counter.most_common())\n        self.word_freq = {\n            **{'<PAD>': 0},\n            **counter,\n        }\n        self.token2id = {\n            **{'<PAD>': 0},\n            **{word: i + 1 for i, word in enumerate(counter)}\n        }\n        self.lfq = np.array(list(self.word_freq.values())) < self.mincount\n        self.hfq = ~self.lfq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SentenceExtraFeaturizerWrapper(object):\n\n    registry = SENTENCE_EXTRA_FEATURIZER_REGISTRY\n    default_config = None\n\n    def __init__(self, config):\n        self.config = config\n        self.featurizers = {\n            k: self.registry[k]() for k in config.sentence_extra_features}\n        self.n_dims = sum(list(f.n_dims for f in self.featurizers.values()))\n\n    @classmethod\n    def add_args(cls, parser):\n        parser.add_argument(\n            '--sentence-extra-features', nargs='+', choices=cls.registry)\n        parser.set_defaults(**cls.default_config)\n\n    def __call__(self, sentence):\n        empty = np.empty((0,))\n        return np.concatenate([empty, *[\n            f(sentence) for f in self.featurizers.values()]], axis=0)\n\n    def fit_standardize(self, features):\n        assert features.ndim == 2\n        self.mean = features.mean(axis=0)\n        self.std = features.std(axis=0)\n        self.std = np.where(self.std != 0, self.std, 1)\n        return (features - self.mean) / self.std\n\n    def standardize(self, features):\n        assert hasattr(self, 'mean'), hasattr(self, 'std')\n        return (features - self.mean) / self.std\n\n\n\n\nclass SentenceExtraFeaturizerPresets(SentenceExtraFeaturizerWrapper):\n\n    default_config = dict(\n        sentence_extra_features=[],\n    )\n    \n    \nclass SentenceExtraFeaturizer(SentenceExtraFeaturizerPresets):\n\n    default_config = dict(\n        sentence_extra_features=['char', 'word'],\n    )    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass ExperimentConfigBuilder(ExperimentConfigBuilderBase):\n\n    default_config = dict(\n        test=False,\n        device=0,\n        maxlen=72,\n        vocab_mincount=5,\n        scale_batchsize=[],\n        validate_from=4,\n    )\n\n    @property\n    def modules(self):\n        return [\n            TextNormalizer,\n            TextTokenizer,\n            #WordEmbeddingFeaturizer,\n            WordExtraFeaturizer,\n            SentenceExtraFeaturizer,\n            #Embedding,\n            #Encoder,\n            #Aggregator,\n            #MLP,\n        ]\n       \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config = ExperimentConfigBuilder().build(args=[])\nprint(config)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_seed(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nset_seed(config.seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, submit_df = load_qiqc(n_rows=config.n_rows)\ndatasets = build_datasets(train_df, submit_df, config.holdout, config.seed)\ntrain_dataset, test_dataset, submit_dataset = datasets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class WordbasedPreprocessor():\n\n    def tokenize(self, datasets, normalizer, tokenizer):\n        tokenize = Pipeline(normalizer, tokenizer)\n        apply_tokenize = ApplyNdArray(tokenize, processes=2, dtype=object)\n        tokens = [apply_tokenize(d.df.question_text.values) for d in datasets]\n        return tokens\n\n    def build_vocab(self, datasets, config):\n        train_dataset, test_dataset, submit_dataset = datasets\n        vocab = WordVocab(mincount=config.vocab_mincount)\n        vocab.add_documents(train_dataset.positives.tokens, 'train-pos')\n        vocab.add_documents(train_dataset.negatives.tokens, 'train-neg')\n        vocab.add_documents(test_dataset.positives.tokens, 'test-pos')\n        vocab.add_documents(test_dataset.negatives.tokens, 'test-neg')\n        vocab.add_documents(submit_dataset.df.tokens, 'submit')\n        vocab.build()\n        return vocab\n\n    def build_tokenids(self, datasets, vocab, config):\n        token2id = lambda xs: pad_sequence(  # NOQA\n            [vocab.token2id[x] for x in xs], config.maxlen)\n        apply_token2id = ApplyNdArray(\n            token2id, processes=1, dtype='i', dims=(config.maxlen,))\n        tokenids = [apply_token2id(d.df.tokens.values) for d in datasets]\n        return tokenids\n\n    def build_sentence_features(self, datasets, sentence_extra_featurizer):\n        train_dataset, test_dataset, submit_dataset = datasets\n        apply_featurize = ApplyNdArray(\n            sentence_extra_featurizer, processes=1, dtype='f',\n            dims=(sentence_extra_featurizer.n_dims,))\n        _X2 = [apply_featurize(d.df.question_text.values) for d in datasets]\n        _train_X2, _test_X2, _submit_X2 = _X2\n        train_X2 = sentence_extra_featurizer.fit_standardize(_train_X2)\n        test_X2 = sentence_extra_featurizer.standardize(_test_X2)\n        submit_X2 = sentence_extra_featurizer.standardize(_submit_X2)\n        return train_X2, test_X2, submit_X2\n'''\n    def build_embedding_matrices(self, datasets, word_embedding_featurizer,\n                                 vocab, pretrained_vectors):\n        pretrained_vectors_merged = np.stack(\n            [wv.vectors for wv in pretrained_vectors.values()]).mean(axis=0)\n        vocab.unk = (pretrained_vectors_merged == 0).all(axis=1)\n        vocab.known = ~vocab.unk\n        embedding_matrices = word_embedding_featurizer(\n            pretrained_vectors_merged, datasets)\n        return embedding_matrices\n'''        \n\ndef build_word_features(self,word_extra_features):\n    return word_extra_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Pipeline(object):\n\n    def __init__(self, *modules):\n        self.modules = modules\n\n    def __call__(self, x):\n        for module in self.modules:\n            x = module(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class WordbasedPreprocessor():\n\n    def tokenize(self, datasets, normalizer, tokenizer):\n        tokenize = Pipeline(normalizer, tokenizer)\n        apply_tokenize = ApplyNdArray(tokenize, processes=2, dtype=object)\n        tokens = [apply_tokenize(d.df.question_text.values) for d in datasets]\n        return tokens\n    \n    def build_vocab(self, datasets, config):\n        train_dataset, test_dataset, submit_dataset = datasets\n        vocab = WordVocab(mincount=config.vocab_mincount)\n        vocab.add_documents(train_dataset.positives.tokens, 'train-pos')\n        vocab.add_documents(train_dataset.negatives.tokens, 'train-neg')\n        vocab.add_documents(test_dataset.positives.tokens, 'test-pos')\n        vocab.add_documents(test_dataset.negatives.tokens, 'test-neg')\n        vocab.add_documents(submit_dataset.df.tokens, 'submit')\n        vocab.build()\n        return vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass PreprocessorPresets(WordbasedPreprocessor):\n\n    def build_word_features(self, word_extra_features):\n        return word_extra_features\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Preprocessor(PreprocessorPresets):\n    def build_word_features(self, word_extra_features):\n        return word_extra_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%cython\nimport numpy as np\ncimport numpy as np\nfrom multiprocessing import Pool\n\n\ncdef class ApplyNdArray:\n    cdef func\n    cdef dtype\n    cdef dims\n    cdef int processes\n\n    def __init__(self, func, processes=1, dtype=object, dims=None):\n        self.func = func\n        self.processes = processes\n        self.dtype = dtype\n        self.dims = dims\n\n    def __call__(self, arr):\n        if self.processes == 1:\n            return self.apply(arr)\n        else:\n            return self.apply_parallel(arr)\n\n    cpdef apply(self, arr):\n        cdef int i\n        cdef int n = len(arr)\n        if self.dims is not None:\n            shape = (n, *self.dims)\n        else:\n            shape = n\n        cdef res = np.empty(shape, dtype=self.dtype)\n        for i in range(n):\n            res[i] = self.func(arr[i])\n        return res\n\n    cpdef apply_parallel(self, arr):\n        cdef list arrs = np.array_split(arr, self.processes)\n        with Pool(processes=self.processes) as pool:\n            outputs = pool.map(self.apply, arrs)\n        return np.concatenate(outputs, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint('Tokenize texts...')\npreprocessor = Preprocessor()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normalizer = TextNormalizer(config)\ntokenizer = TextTokenizer(config)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset.tokens, test_dataset.tokens, submit_dataset.tokens = \\\n    preprocessor.tokenize(datasets, normalizer, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint('Build vocabulary...')\nvocab = preprocessor.build_vocab(datasets, config)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint('Build sentence extra features...')\nsentence_extra_featurizer = SentenceExtraFeaturizer(config)\ntrain_dataset._X2, test_dataset._X2, submit_dataset._X2 = \\\n    preprocessor.build_sentence_features(\n        datasets, sentence_extra_featurizer)\n[d.build(config.device) for d in datasets]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}